---
title: L'√©quipe Backend
date: 2020-12-17T09:00:00
hero: /post/2020/team/backend/images/hero.jpg
excerpt: Welcome to the other side
authors:
  - ldechoux
description: "D√©couvrez l'√©quipe backend qui se cache derri√®re MYTF1"
---

## Avant propos
Cet article a pour objectif de pr√©senter l'√©quipe Backend et n'est en aucun cas une pr√©sentation technique d√©taill√©e des entrailles de MYTF1. Les aspects techniques seront abord√©s en d√©tail dans des articles d√©di√©s. Ici, nous nous concentrerons sur la composition de l'√©quipe, son histoire et partagerons avec vous quelques unes des d√©cisions que nous avons prises ces derni√®res ann√©es. Bonne lecture.

## Qui sommes nous ?
Int√©gr√©e au sein de e-TF1 (antenne digitale du groupe TF1) l‚Äô√©quipe Backend a pour objectif de r√©pondre aux probl√©matiques suivantes :

- G√©rer la mise en ligne et l‚Äôanimation √©ditoriale de notre contenu
- Stocker et restituer les donn√©es utilisateurs (historique et progression de lecture, programme favoris, bookmarks etc.)
- Exporter/partager notre catalogue de contenu avec nos partenaires (FAI, Salto, etc.)
- Fournir des API aux fronts pouvant supporter de fortes charges

Elle est compos√©e d‚Äôune dizaine de personnes ayant des profils (d√©veloppeur, product owner, lead tech, internes ou externes) et des niveaux d‚Äôexp√©rience (d√©butant, exp√©riment√©, stagiaire, alternant) diff√©rents. Depuis 2018 nous avons fait le choix d‚Äôinvestir fortement dans le langage Go qui repr√©sente aujourd‚Äôhui la quasi int√©gralit√© de notre base de code.

## Architecture et technologies
Nous avons fait le choix d‚Äôune architecture micro-services. Les diff√©rentes composantes m√©tier sont r√©parties en services d√©di√©s, qui communiquent principalement via GRPC.
Voici une liste non exhaustives de nos briques m√©tier :

- CMS API : d√©di√©e √† l‚Äôanimation √©ditoriale de notre contenu
- Catalog API : d√©di√©e √† la gestion de notre catalogue de contenu
- User API : d√©di√©e √† la gestion des donn√©es utilisateur
- Reco API : d√©di√©e √† la recommandation de contenu
- SEO API : d√©di√©e  aux probl√©matiques SEO
- Auth API : d√©di√©e √† l‚Äôidentification de nos utilisateurs

Pour les applications MYTF1, nous avons fait le choix d‚Äôexposer √† travers une API GraphQL une vision consolid√©e de ces services.
En effet, l‚ÄôAPI GraphQL agit comme une API Gateway et se charge d‚Äôexposer un mod√®le de donn√©es coh√©rent et unifi√© qui r√©pond aux besoins exprim√©s par les √©quipes produit/m√©tier.
Elle a √©t√© con√ßue et cr√©√©e avec une vision multi-√©cran et doit √™tre capable de fonctionner aussi bien pour nos applications Web, que pour les applications mobiles ou encore les box op√©rateurs.

Pour r√©pondre aux diff√©rents challenges auxquels nous faisons face, nous avons choisi les technologies suivantes : 

- Langage : [Go](https://golang.org/)
- Base de donn√©es : [MongoDB](https://www.mongodb.com/), [Elasticsearch](https://www.elastic.co/), [DynamoDB](https://aws.amazon.com/dynamodb/), [Redis](https://redis.io/)
- Event/Message broker : [RabbitMQ](https://www.rabbitmq.com/), [Kafka](https://kafka.apache.org/)
- Frameworks Fronts : [Vue.js](https://vuejs.org/), [React](https://reactjs.org/)
- Formats des API : [GRPC](https://grpc.io/), [GraphQL](https://graphql.org/), [Rest](https://fr.wikipedia.org/wiki/Representational_state_transfer)
- Infrastructure : [AWS](https://aws.amazon.com/), [Kubernetes](https://kubernetes.io), [Docker](https://www.docker.com/), [Linkerd](https://linkerd.io/)
- Monitoring : [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/)
- CI/CD : [Jenkins](https://www.jenkins.io/)

Cette liste, bien que fournie, peut-√™tre amen√©e √† √©voluer en fonction des futurs besoins qui se pr√©senteront.
En effet, une des forces de l‚Äô√©quipe est de savoir se remettre en question et faire table rase du pass√©. C‚Äôest ce que nous allons voir dans le paragraphe suivant.

## Un peu d‚Äôhistoire
### 2018 : Nouvelle exp√©rience IPTV
D√©but 2018 MYTF1 se lance dans un projet radical de transformation de l‚Äôexp√©rience utilisateur sur les box op√©rateurs (IPTV).
Un cahier des charges est d√©fini et de nouveaux enjeux apparaissent :

- Permettre une navigation fluide du contenu
- Possibilit√©s d‚Äô√©ditorialisation avanc√©es
- Recommandation de contenu personnalis√©e
- Gestion de l‚Äôhistorique et de la reprise de lecture
- Mise en favoris des programmes

√Ä ce stade, nous sentons bien que le socle technologique existant a atteint ses limites et qu‚Äôil faut envisager des changements drastiques.
Une petite √©quipe est mont√©e pour relever ce d√©fi. Elle deviendra plus tard l'√©quipe Backend.

Nous ferons alors plusieurs choix structurants :

- Mise en place d‚Äôune API GraphQL pour exposer les donn√©es au front
- Utilisation de cache in-memory (non partag√©) au niveau de l'API GraphQL
- D√©coupe des diff√©rents besoins en micro-services GRPC d√©di√©s
- D√©normalisation des donn√©es catalogue et √©ditoriales dans Elasticsearch
- Gestion de la session utilisateur dans Redis
- Ecriture asynchrone des donn√©es utilisateur
- Utilisation d‚Äôun token JWT pour identifier l‚Äôutilisateur

D√®s le d√©but, m√™me si le projet est centr√© sur l‚ÄôIPTV, nous avons la volont√© de cr√©er un nouveau socle technique qui sera capable d‚Äôadresser tous les √©crans (Web et applications mobiles compris).
En effet, √† date, il n‚Äôy a pas r√©ellement de socle commun et chaque √©cran est trait√© s√©par√©ment (avec ses propres solutions, technologies et choix techniques).
Nous avons toujours gard√© cette id√©e dans un coin de nos t√™tes et elle a pilot√© beaucoup de nos d√©cisions.

S‚Äôen suit alors une phase intensive de conception et d√©veloppement, en effet les d√©lais sont serr√©s (mise en production attendue pour Juin 2018) et nous avons beaucoup de travail √† abattre (autant c√¥t√© back que c√¥t√© front).
L'application sera finalement lanc√©e en Juillet 2018, pour les utilisateurs de freebox.

La premi√®re mouture de notre nouvelle architecture backend est enfin pr√™te :

![2018 - Premi√®re version de la nouvelle architecture backend](images/archi_2018.svg "2018 - Premi√®re version de la nouvelle architecture backend")

Concr√©tement, nous avons deux sources de donn√©es principales, le CMS et les fichiers de recommandation (au format [Parquet](https://parquet.apache.org/)). Via des *indexers*, nous d√©normalisons r√©guli√®rement ces donn√©es dans des bases Elasticsearch qui sont ensuite expos√©es via des API GRPC d√©di√©es (Catalog et Reco sur le sch√©ma). Pour les donn√©es utilisateur, elles sont √©galement stock√©es dans des bases Elasticsearch. Lorsqu'un utilisateur se connecte, les donn√©es qui lui sont associ√©es sont copi√©es dans des instances Redis qui agissent comme un cache de session. Toutes les √©critures sont maintenues √† jour de mani√®re synchrone dans Redis puis propag√©es de mani√®re asynchrone vers nos bases Elasticsearch via des notifications RabbitMQ.
Les donn√©es utilisateur sont accessibles via des API GRPC d√©di√©es (History et Favorites sur le sch√©ma). Au dessus de ces services nous avons notre API GraphQL qui se charge d'unifier les donn√©es des diff√©rentes briques et en exposer une vision consolid√©e aux fronts (les box op√©rateurs dans le cas pr√©sent). Pour l'identification des utilisateurs, nous avons fait le choix du token JWT. Un service d√©di√© se charge de g√©n√©rer un token (sur demande du front) qui est ensuite propag√© dans tous les appels GraphQL puis vers les services concern√©s (History et Favorites par exemple). Ainsi nous pouvons facilement identifier l'utilisateur √† l'origine de la requ√™te et retrouver, par exemple, son historique de lecture.

Niveau infrastucture, toutes nos applications sont packag√©es sous forme d'image Docker qui sont ensuite d√©ploy√©es sur notre cluster Kubernetes maison (g√©r√© par notre √©quipe OPS). √Ä ce stade, nous avions √©galement fait le choix de ne pas utiliser de cache HTTP entre les box et le GraphQL. En effet les r√©ponses m√©langent √† la fois donn√©es publiques (ex : catalogue) et priv√©es (ex : historique de lecture) et se pr√™tent donc mal √† l'exercice. Seule solution, faire en sorte de tenir la charge en dimenssionnant correctement notre infrastructure (phase de bench) et en optimisant les applications (en utilisant, par exemple, des caches in-memory c√¥t√© GraphQL pour les donn√©es publiques).

Un deuxi√®me jalon important marquera l'ann√©e 2018, avec la mise √† disposition de notre nouvelle application IPTV sur les box Android de Bouygues Telecom. Mais pour nous, ce n'est que le d√©but...

### 2019 : De l'IPTV √† l'OTT
Mi-2018, √©merge chez e-TF1 l'envie de refondre les applications MYTF1 web et mobile (dites [OTT](https://fr.wikipedia.org/wiki/Service_par_contournement)). Au del√† de l'aspect esth√©tique il y a une v√©ritable volont√© de repenser le produit et le recentrer autour d'axes strat√©giques pr√©cis. Le second semestre 2018 est mis √† profit pour d√©finir pr√©cis√©ment les contours de ce nouveau produit. Au terme de cette r√©flexion plusieurs priorit√©s sont d√©finies :

- Un nouveau design pour les applications web et mobile
- Une exp√©rience de lecture vid√©o irr√©prochable
- Mettre la personnalisation au centre de l'exp√©rience MYTF1
- Am√©liorer les outils d'√©ditorialisation
- Proposer une nouvelle offre de contenus (AVOD)
- Gestion de la reprise de lecture cross-device

Le choix de notre nouvelle architecture backend comme socle de cette nouvelle vision du produit se fait naturellement. C'est un nouveau challenge pour nous et l√† encore beaucoup de travail nous attend.

En parall√®le, le second semestre 2018 nous permet de renforcer l'√©quipe avec de nouveaux membres. Nous en profitons pour retravailler et am√©liorer certains aspects techniques de notre architecture pour pr√©parer le futur. C'est √©galement l'occasion de r√©fl√©chir aux diff√©rents choix techniques que nous allons devoir faire pour g√©n√©raliser le backend √† l'ensemble des √©crans MYTF1. Les principaux choix effectu√©s sont les suivants :

- Migration vers AWS
- Rendre l'API GraphQL publique (expos√©e sur internet, jusqu'alors elle est expos√©e sur des IP priv√©es pour les diff√©rents op√©rateurs IPTV)
- Gestion du cache HTTP et mise en place des *Persisted Queries* GraphQL
- Permettre la recommandation de contenu pour les utilisateurs non connect√©s (introduction des personas)
- Refonte du CMS
- Refonte m√©diath√®que et proxy image
- Gestion du moteur de recherche

Le gros des travaux commence r√©ellement fin 2018 et concerne l'ensemble des √©quipes (web, mobile, publicit√©, player, OPS). L√† encore le planning est ambitieux, la mise en production du nouveau MYTF1 est pr√©vue pour Avril/Mai 2019. Finalement, le lancement aura lieu le 11 Juin 2019.

Notre architecture cuv√©e 2019 ressemble alors √† √ßa :

![2019 -  Sch√©ma d'architecture backend](images/archi_2019.svg "2019 - Sch√©ma d'architecture backend")

Comme vous pouvez le voir, nous avons enrichi le socle de plusieurs nouveaux composants. Une grosse partie de nos efforts s'est concentr√©e autour de nos outils internes. En particulier le CMS que nous avons enti√®rement refondu pour r√©pondre aux nouveaux besoins des √©quipes √©ditoriales. Nous avons fait le choix de cr√©er un CMS headless (ie : l'aspect templating est totalement g√©r√© par le front) autour de MongoDB et Elasticsearch (pour le stockage des donn√©es) et de Vue.js (pour la partie interface graphique). Pour la gestion des images nous avons √©galement cr√©√© une nouvelle m√©diath√®que profitant des possibilit√©s de stockages offertes par S3. De plus nous avons introduit un nouveau composant *Image proxy* dont le but est de permettre la mise √† l'√©chelle automatique des images (il est possible d'obtenir une version d'une image √† une r√©solution diff√©rente de celle d'origine) et leur conversion dans plusieurs formats (webp, jpeg, png, etc.). Nous avons √©galement ajout√© une m√©canique de *crop intelligent* qui pr√©serve les parties importantes d'une image (d√©tection des visages, entropie, etc.). Pour ce faire nous nous appuyons sur [OpenCV](https://github.com/opencv/opencv).

![D√©monstration crop intelligent](images/smart_crop.jpg "D√©monstration crop intelligent")

Nous avons √©galement ajout√© un composant d√©di√© au moteur de recherche accessible dans les fronts. Il s'appuie sur les capacit√©s de recherche *full-text* offertes par Elasticsearch. Une autre nouveaut√© introduite avec la refonte MYTF1 est la notion de persona. L'id√©e est de pouvoir personnaliser/adapter le contenu propos√© √† chaque utilisateur (m√™me lors de sa premi√®re connexion au site ou au premier lancement de l'application mobile). Via une API fournie par l'√©quipe data, nous affectons aux utilisateurs une persona (en fonction de diff√©rents crit√®res : utilisateur authentifi√© ou non, appareil utilis√©, heure de connexion, dernier contenu consult√©, etc.) qui va impacter la mani√®re dont le contenu est pr√©sent√©. Pour ce faire, nous calculons r√©guli√®rement des tops (vid√©os les plus vues, programmes les plus consult√©s) pour chaque persona. Ces tops sont ensuite utilis√©s pour, par exemple, modifier (en fonction de la persona attribu√©e √† l'utilisateur) l'ordre d'affichage des programmes sur la home MYTF1.

Enfin une des grosses nouveaut√©s introduite en 2019 est l'utilisation des *persisted queries* au niveau de notre API GraphQL. Le principe est relativement simple. Au lieu d'envoyer une requ√™te classique (POST + body), les fronts appellent le GraphQL via des identifiants de requ√™te (GET + query parameters). L'int√©r√™t est double. Premi√®rement on profite facilement des possibilit√©s de cache offertes par Cloudfront et deuxi√®mement il nous est possible de verrouiller le GraphQL en production (ne sont autoris√©es que les requ√™tes pr√©alablement d√©finies dans notre r√©f√©rentiel). Si ce sujet vous int√©resse, il est abord√© en d√©tail dans cet article : [GraphQL et persisted queries](/post/2020/architecture/graphql-and-persisted-queries/).

Le dernier challenge relev√© dans le cadre de la refonte MYTF1 a √©t√© la migration vers AWS de l'ensemble de notre socle backend. Nous h√©bergions auparavant l'ensemble des services au sein d'un datacenter g√©r√© par notre √©quipe infrastucture. Suite √† un travail conjoint avec cette derni√®re, l'ensemble de nos services a maintenant bascul√© dans le cloud nous offrant ainsi beaucoup d'avantages (services manag√©s, mise √† l'√©chelle automatique, etc.).

Le premier semestre 2019 f√ªt donc riche pour l'ensemble des √©quipes. Nous avons principalement consacr√© le second semestre √† enrichir le produit (fourniture de nouvelles fonctionnalit√©s) et poursuivi le d√©ploiement IPTV √† d'autres op√©rateurs (SFR et VIDEOFUTUR). Nous avons √©galement, avec le support de l'√©quipe OPS, g√©n√©ralis√© la mise en oeuvre de [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) sur l'ensemble de nos briques applicatives.

### 2020 : Vers une architecture "temps r√©el"
D√©but 2020, bonnes r√©solutions obligent, nous avons dress√© un bilan de notre architecture, identifi√© les faiblesses et imagin√© des solutions pour y rem√©dier. Un travail qui nous a permis de dresser une feuille de route pour 2020 qui s‚Äôarticule autour de deux grands axes.

Tout d‚Äôabord, l‚Äôaspect temps r√©el. Comme √©voqu√© dans les paragraphes pr√©c√©dents, nous nous basons sur des *indexers* pour d√©normaliser les donn√©es provenant du CMS dans des instances Elasticsearch. C‚Äôest une mani√®re de faire relativement simple qui, bien que fonctionnelle, introduit une latence entre les mises √† jour faites par les √©quipes √©ditoriales et leurs mises en ligne effectives sur les fronts. En effet nos *indexers* tournent r√©guli√®rement ce qui impose, suite √† une modification, d‚Äôattendre l‚Äôindexation suivante pour que celle-ci soit mise en ligne.

Vient ensuite l‚Äôaspect performance. Depuis la refonte des produits MYTF1 (autant IPTV que OTT) les audiences sont en hausse. De plus nous continuons √† d√©ployer notre nouvelle application IPTV chez d‚Äôautres op√©rateurs (Orange entre autre) ce qui am√®ne une charge suppl√©mentaire sur nos services backend. Depuis 2 ans nous avons mis√© sur diff√©rentes m√©caniques de mises en cache (CDN, cache HTTP, cache in-memory, etc.) pour √™tre en mesure d‚Äôabsorber la charge g√©n√©r√©e par l‚Äôactivit√© des utilisateurs. Ces diff√©rentes couches de cache sont de moins en moins √©videntes √† maintenir et participent √©galement √† la latence des mises en ligne des contenus √©voqu√©e pr√©c√©demment.

La solution que nous avons imagin√©e est de basculer vers une architecture √©v√©nementielle :

![2020 -  Sch√©ma d'architecture backend](images/archi_2020.svg "2020 - Sch√©ma d'architecture backend")

Pour ce faire, nous avons d√©cid√© de nous appuyer sur Kafka (plus pr√©cis√©ment, l'offre manag√©e [MSK](https://aws.amazon.com/msk/) d'AWS). Plusieurs sources d'√©v√©nements ont √©t√© identifi√©es :

- Le CMS pour la partie contenu/√©dito, nous nous appuyons sur les [Change Streams](https://docs.mongodb.com/manual/changeStreams/) MongoDB pour cela
- Les fichiers parquet de recommandation que nous injectons dans des topics Kafka
- Les actions des utilisateurs (lecture vid√©o, enregistrement de l'avanc√©e de lecture, mise en favoris, etc..)

Pour la partie CMS, l'id√©e est de pousser toutes les modifications faites en base dans des topics Kafka d√©di√©s (voir notre projet open source [kafka-mongo-watcher](https://github.com/etf1/kafka-mongo-watcher)). Ensuite ces √©v√©nements sont trait√©s, transform√©s puis stock√©s (voir notre projet open source [kafka-transformer](https://github.com/etf1/kafka-transformer)) dans des instances [Elasticache Redis](https://aws.amazon.com/fr/elasticache/redis/). Nous maintenons alors √† jour, en quasi temps r√©el, notre catalogue de contenu dans un cache partag√© sur lequel nous avons directement branch√© nos instances GraphQL. Deux cons√©quences : nous ne sommes plus d√©pendants des *indexers* de donn√©es et nous avons supprim√© la couche de cache in-memory (non partag√©e) de notre API GraphQL. De plus nous avons d√©normalis√© les donn√©es dans Redis de telle mani√®re que le service catalogue devient superflu. Ainsi nous gagnons sur les deux tableaux (latence li√©e √† l'indexation et performance de l'API GraphQL).

Pour traiter certains cas particuliers, nous avons recours √† [Goka](https://github.com/lovoo/goka) pour, par exemple, permettre la jointure et l'aggr√©gation de donn√©es en provenance de plusieurs types d'√©v√©nements diff√©rents (exemple : jointure entre les mises √† jour des programmes et des vid√©os pour produire des curations √©ditoriales qui sont ensuite stock√©es dans Redis). Enfin, nous avons introduit un composant *scheduler* dont l'objectif est de produire des √©v√©nements temporels sur lesquels le syst√®me va pouvoir r√©agir (exemple : expiration d'une vid√©o).

Pour la partie utilisateur, nous avons conserv√© globalement la m√™me architecture qu'avant mais en la modernisant : 

- Fusion au sein d'une seule API des donn√©es utilisateurs
- Bascule vers DynamoDB (√† la place des instances Elasticsearch)
- Bascule sur Kafka (√† la place de RabbitMQ)
- Migration de l'instance Redis vers Elasticache

Enfin pour la partie recommandation nous avons :

- Inject√© les fichiers parquet directement dans Kafka
- Remplac√© la base Elasticsearch par une base DynamoDB pour le stockage √† froid
- Ajout√© une instance Redis qui agit comme cache partag√©
- Introduit, gr√†ce √† notre √©quipe data, une nouvelle API de recommandation temps r√©el

Dans les deux cas, nous profitons maintenant des possibilit√©s de mise √† l'√©chelle automatique de DynamoDB et des performances accrues des services manag√©s MSK et Elasticache.

Toutes ces modifications permettent donc des gains notables sur la performance, la mise √† l'√©chelle et la r√©duction des latences de notre architecture. Mais nous sommes encore en phase transitoire et d'autres √©volutions sont d√©j√† pr√©vues, notamment autour de la m√©diath√®que et du pr√©-chargement des donn√©es dans notre CDN, mais aussi sur le calcul des tops vid√©os et programmes en temps r√©el.

## Conclusion
Comme vous avez pu le constater √† la lecture de cet article, les derni√®res ann√©es ont √©t√© riches pour l'√©quipe Backend. Je tiens personnellement √† remercier toute l'√©quipe pour son travail, ses comp√©tences et sa capacit√© √† remettre en question ses choix pour oeuvrer √† l'am√©lioration continue de notre architecture (le tout dans une super ambiance üòÄ). Bien que dense, cet article ne fait qu'effleurer certains aspects techniques. Nous les d√©velopperons dans de futurs articles qui, nous l'esp√©rons, r√©ussiront √† capter votre attention.

## Remerciements
Merci aux relecteurs de l'article : Sabine, D√©borah, Guillaume, Richard, Thierry et Vincent.