---
title: Tour d'horizon technique
date: 2020-09-01
hero: /post/2020/architecture/presentation/images/hero.jpg
excerpt: D√©couvrez les coulisses techniques de MYTF1
authors:
  - rpinsonneau
---

# MYTF1 qu√©saco ?

[MYTF1](https://www.tf1.fr/) est le service de replay du [groupe TF1](https://www.groupe-tf1.fr). Il permet √† nos utilisateurs de voir ou revoir en streaming les programmes des cha√Ænes suivantes : [TF1](https://www.tf1.fr/tf1), [TMC](https://www.tf1.fr/tmc), [TFX](https://www.tf1.fr/tfx), [TF1 S√©ries Films](https://www.tf1.fr/tf1-series-films) et [LCI](https://www.lci.fr/). Il est disponible sur la plupart des √©crans: Web, Mobile (iOS, Android) et sur les box des principaux op√©rateurs (IPTV). Ce service gratuit tire principalement ses revenus de la publicit√©.

MYTF1 englobe un large spectre de sujets :

- le streaming vid√©o et l'encodage des contenus
- le ciblage de la publicit√©
- la recommendation de contenu (la data)
- l'animation des contenus (l'√©ditorialisation)
- la gestion des donn√©es utilisateurs (historique de lecture, mise en favoris, etc...)

# Les technos utilis√©es

MYTF1 existe depuis 2011 et a √©t√© depuis plusieurs fois refondu from scratch.

| P√©riode            | Technos                                              |
| ------------------ | ---------------------------------------------------- |
| 2011 - 2015        | PHP, MySQL                                           |
| 2015 - 2019        | NodeJS, PostgreSQL                                   |
| 2019 - Aujourd'hui | Go, GraphQL, gRPC, MongoDB, Kafka, Redis, Kubernetes |

## Backend

Aujourd'hui le backend est constitu√© d'un ensemble de micro-services √©crits en [Go](https://golang.org). Apr√®s la p√©riode [NodeJS](https://nodejs.org/), nous avons d√©cid√© de retourner √† un language fortement typ√©. La fa√ßon dont Go g√®re la concurrence (go routine) permet de tenir le fort traffic de MYTF1 et est particuli√®rement adapt√© a un √©cosysteme **kubernetes** :

- emprunte m√©moire faible
- d√©marrage rapide (binaire compil√©)
- taille des images docker r√©duite
- id√©al pour des services HTTP / [gRPC](https://grpc.io)

C'est √©galement un language rapide √† apprendre.

### GraphQL

L'API [GraphQL](https://graphql.org) est une brique centrale pour MYTF1. Nous l'utilisons comme source de donn√©es des diff√©rents fronts. Les avantages sont les suivants :

- pas de service sp√©cifique par √©cran, chaque front peut requ√™ter ce dont il a besoin uniquement
- le GraphQL joue le r√¥le d'API gateway, c'est lui qui rassemble les donn√©es des micro-services sous jacents
- contrat d'interface auto document√© entre le back et les fronts

### Les bases de donn√©es

Nous utilisons [MongoDB](https://www.mongodb.com) et [PostgreSQL](https://www.postgresql.org) pour les bases de donn√©es de r√©f√©rence.
Ces donn√©es sont ensuite d√©normalis√©es dans des cluster [Redis](https://redis.io). Nous avons adopt√© une architecture "event driven" en nous appuyant sur [Kafka](https://kafka.apache.org) pour maintenir une synchronisation constante entre ces bases de donn√©es.

## Web

Le front MYTF1 repose sur une SPA en [Reactjs](https://fr.reactjs.org) et un serveur expressjs pour le SSR (server side rendering) pour assurer un bon r√©f√©rencement. Nous utilisons react car la partie SSR est √©prouv√©e ainsi que le large √©cosyst√®me open source.

### Applicatif front :

La stack est principalement ax√©e sur les performances et le SEO. React via React-dom/server permet de g√©n√©r√© du HTML cote serveur qui sera ensuite "hydrater" cote client pour assurer une bonne UX.

**TypeScript :**
Nous utilisons fortement [Typescript](https://www.typescriptlang.org) pour que l'appropriation du code soit rapide, de plus, et comme tests "statiques" pour s'assurer que l'on envoie bien le bon type de donn√©es aux fonctions

**GraphQL & Apollo :**
[Apollo](https://www.apollographql.com) nous permet de consommer l'API GraphQL et fonctionne aussi cote serveur.
Via GraphQL code generator, on genere toute nos composants/hooks Apollo via nos queries/mutations en typescript, ce qui permet encore une fois de s'assurer que ces requ√™tes sont valides.

**Helmet :**
[Helmet](https://github.com/staylor/react-helmet-async) est la librairie (react-helmet-async et non pas react-helmet) nous permet d'enrichir au fur et a mesure des composants rendu les balises metas qui aident a la compr√©hension des robots de moteurs de recherches du contenu de nos pages.

### Performance et Qualit√© :

**Webpack & Lazyloading :**
_brouillon : Nous utilisons [Webpack](https://webpack.js.org) pour nos ressources statiques‚Ä¶( modules, chunk...), (code splitting) splitter nos builds en plusieurs paquets, charg√© √† la demande (√† compl√©ter ....)_

**Jest / React Testing Library :**
Nous utilisons [Jest](https://jestjs.io) pour nos tests unitaires, ce qui nous permet de v√©rifier la non-r√©gression du front MYTF1, tout au long du d√©veloppement de nos features et de garantir la fonctionnalit√© de composants complexes.

## APP

Les applications mobiles sont natives et cod√©es en [Swift](https://swift.org) (iOS) et [Kotlin](https://kotlinlang.org) (Android) impl√©mentant une architecture modulaire multi couches.

**Couche Networking:**
Un client GraphQL [Apollo](https://github.com/apollographql/apollo-ios) int√©gr√© dans l'application nous permet de consommer l'API backend GraphQL.

**Couche Core:**
Couche contenant la logique m√©tier et les mod√®les utiliser dans l'application.

**Couche Pr√©sentation:**
Impl√©mentant une architecture MVI unidirectionnel qui repr√©sente une √©volution de l'architecture MVVM avec des bindings en RxSwift & RxJava. Les avantages d'une telle architecture est un flux de donn√©es plus facile √† suivre et √† debugger.


## IPTV

Les techno utilis√©es sur l'iptv sont tr√®s vari√©es et d√©pendent du mod√®le de box, globalement on retrouve trois familles:

- **HTML/JS**, principalement SFR et Orange
- **QT/QML**, tr√®s utilis√© par Free
- **Android**, notamment sur Bouygues et Free

Dans le cas des box Android, un moteur QT/QML tourne dans l'application afin de r√©utiliser le code QT/QML.

## Le player

Nous developpons notre propre player pour diff√©rentes plateformes :

- le Web (JS)
- iOS (Swift)
- Android (Kotlin) pour les applications mobiles et certaines box op√©rateur

TODO a voir avec Guillaume

## Le CMS

Nous avons √©galement d√©velopp√© un CMS maison qui permet √† l'√©quipe √©dito d'animer le contenu de MYTF1. Il est d√©velopp√© en [Vue.js](https://vuejs.org).

# L'architecture backend

## Event driven

TODO un sch√©ma de l'archi globale + explication event driven

# La performance et la QOS

La performance est un sujet critique pour MYTF1. Lors de grands √©v√®nements tels que la coupe du monde de football ou de la diffusions de programmes f√©d√©rateurs comme The Voice ou Koh-Lanta, le service doit tenir la charge face √† plusieurs centaines de milliers d'utilisateurs simultan√©s qui peuvent se connecter √† seulement quelques minutes d'intervalle pour, par exemple, suivre un live.

## La gestion du cache

### Les niveaux de cache

Pour tenir la charge, diff√©rents niveaux de caches sont utilis√©s:

- le cache apollo, permet d'avoir un cache coh√©rent au niveau d'un front pour un utilisateur (donn√©es priv√©es et publiques)
- les CDN ([cloudfront](https://aws.amazon.com/cloudfront/)) permettent de mettre en cache les donn√©es publiques et le flux vid√©o cot√© backend
- les base de donn√©es Redis permettent de mettre en place une session pour les donn√©es priv√©es ou d'avoir un vision d√©normalis√©e des donn√©es publiques

### GraphQL et les persisted queries

Historiquement, MYTF1 s'appuie beaucoup sur le cache des CDN. En effet, une grande partie des donn√©es sont publiques, notamment toutes les informations li√©es au catalogue vid√©o. Avec GraphQL il n'est pas naturel d'utiliser ce type de cache. Par nature, la combinatoire des requ√™tes et le fait de pouvoir m√©langer donn√©es priv√©es et publiques ne permet pas de cacher.

Nous avons r√©utilis√© la m√©canique de "persisted queries" d'apollo que nous avons l√©g√®rement modifi√©e. Celle-ci consiste √† sauvegarder le body de la requ√™te dans une base de donn√©es. Le client apollo n'envoit qu'un ID (en g√©n√©ral un hash du body de la requ√™te) dans une requ√™te GET pour int√©rroger le GraphQL. De cette fa√ßon il est plus simple de mettre en place du cache CDN. Ce fonctionnement est activ√© en PROD, sur les environnements de developpement il reste desactiv√© pour garder toute la souplesse de GraphQL. Les body des requ√™tes sont sauvegard√©s en base de donn√©es au moment du build par notre CI/CD.

![Diagramme explicatif du fonctionnement des persisted queries](images/persisted-queries.svg "Diagramme explicatif du fonctionnement des persisted queries")

Les avantages sont les suivants:

- le GraphQL est v√©rouill√© en PROD on ne peut pas explorer le schema avec des requ√™tes non pr√©sentes dans le code
- le GraphQL n'est pas v√©rouill√© sur les environnment hors PROD, on garde donc toute la souplesse de l'API pour les developpeurs
- les requ√™tes publiques sont connues √† l'avance et associ√©es √† un TTL qui est transmis dans un header Cache-Control et donc exploit√©s par les CDN

### Le cache de donn√©es priv√©es

Il est necessaire, pour exploiter efficacement le cache, de s√©parer les call qui pr√©sentent des donn√©es priv√©es. Une partie de l'intelligence est d√©port√©e sur le front. Par exemple, la liste des vid√©o mises en favoris par un utilisateur, est r√©cup√©r√©e en d√©but de session. L'√©cran est construit √† partir des donn√©es publiques, les favoris pr√©c√©demments r√©cup√©r√©s viennent enrichir l'√©cran, en affichant un coeur sur la vignette vid√©o. Cette logique est plus complexe mais garanti que m√™me lors d'une indisponibilit√© du backend, le front pourra afficher un √©cran dans un mode d√©grad√© (√† partir du stale cache CDN).

<!--more-->

Nous nous appuyons √©galement sur des cache Redis, en d√©but de sessions, les donn√©es d'un utilisateur sont remont√©es dans ce type de cache, les call ult√©rieurs sont alors moins couteux.

<!--more-->

Enfin, l'utilisations d'un token JWT, permet d'identifier un utilisateur sans sollicitation d'un service ou d'une base de donn√©es, le token est transmis du GraphQL aux micro services sous jacents qui peuvent alors v√©rifier eux m√™me la validit√©e du token (par simple v√©rification de la signature).

## Le monitoring

Nous utilisons l'outil [Grafana](https://grafana.com), associ√© √† [Prometheus](https://prometheus.io), pour faire le monitoring de nos applicatifs et de notre infrastructure. [Graylog](https://www.graylog.org) permet quand √† lui de r√©cup√©rer les logs d'execution. Nous enviseagons d'utiliser [Jaeger](https://www.jaegertracing.io) qui permettrait de restituer une vision coh√©rente des services en terme de monitoring et de logs.

![Exemple de dashboard Grafana](images/grafana-graphql.png "Exemple de dashboard Grafana")

# La vid√©o
La vid√©o est un domaine assez large, avec pas mal d'acronymes et de formats exotiques. Nous allons y aller progressivement üòâ
## Plusieurs types de vid√©os et modes de diffusion
Nous distinguons 2 types de vid√©os:
- les flux live (chaines TF1, TMC, TFX, TF1 S√©ries Films, LCI et lives √©venementiels) qui proviennent d'un _encodage_ en temps r√©el d'un flux vid√©o "broadcast" vers un format de diffusion vid√©o "informatique". Nous appelerons cette partie "live"
- les replays, extraits, spots publicitaires et bonus digitaux que nous regrouperons ici sous l'appelation "replay", et qui subissent des _transcodages_ vers diff√©rents formats pour les diff√©rents √©crans de diffusion (dont les capacit√©s varient)
MYTF1 diffuse de la vid√©o de 2 mani√®res diff√©rentes:
- en OTT (_over-the-top_, terme consacr√© pour la diffusion via Internet) via notre infrastructure ou des services tiers que nous payons (CDN - Content Delivery Networks). Ici notre enjeu est d'offrir la meilleure exp√©rience au plus grand nombre, en terme de qualit√© visuelle, de latence et d'accessibilit√©, tout en minimisant nos co√ªts de diffusion, sans oublier la protection des contenus des ayants droit.
- via des partenaires qui assurent l'√©ventuel transcodage et la diffusion (IPTV - portails des box - et Salto).
Au niveau des formats de diffusion OTT, nous supportons les formats suivants:
- HLS ("HTTP Live Streaming", sur apps iOS et Safari Mobile)
- DASH ("Dynamic Adaptive Streaming over HTTP", sur le reste)
- MP4 (pour les courts spots de pub des replays)
HLS et DASH sont des formats de diffusion adapt√©s √† la diffusion sur Internet: la vid√©o est transcod√©e en diff√©rentes qualit√©s et segment√©e en bouts de quelques secondes, ce qui permet au player de s'adapter en cours de visionnage en t√©l√©chargeant la qualit√© la plus appropri√©e √† sa capacit√© actuelle de t√©l√©chargement.
Pour la protection contre la copie, nous utilisons sur les replays en DASH les DRM Widevine (DRM Google: players sous Chrome, Firefox, Android ...) et Playready (DRM Microsoft, donc players sous Edge) et sur les replays en HLS la DRM Fairplay (Apple)
Les diff√©rents formats possibles pour une vid√©o ne sont pas stock√©s de mani√®re permanente, ils sont g√©n√©r√©s √† la demande et mis en cache.
Au niveau des formats de compression OTT, nous utilisons le codec H.264 pour la vid√©o, et le codec AAC pour l'audio.
## Plusieurs activit√©s dans la gestion de la vid√©o
La vid√©o chez MYTF1 peut se d√©composer en 2 grandes parties:
### Gestion des m√©tadonn√©es live et replay (titre, r√©sum√©, dates de diffusion antenne et/ou de disponibilit√© sur MYTF1, ...) et des mises en ligne
- un backoffice √©ditorial de commande de "replay" (d√©velopp√© en interne)
- mise √† disposition de ces informations aux autres services de MYTF1 et aux partenaires via diff√©rentes API et files de messages
- services pour les players (r√©cup√©ration des m√©tadonn√©es et de l'URL de diffusion, protection de certains contenus via DRM - Digital Rights Management)
### Gestion des donn√©es vid√©o live et replay
- ingestion (encodage/transcodage, gestion des sous-titres √©ventuels, packaging - pr√©paration √† la diffusion OTT)
- envoi aux partenaires, pour les vid√©os (le live IPTV est g√©r√© par TF1)
- diffusion OTT (g√©n√©ration des formats HLS/DASH/MP4 √©ventuellement DRM-is√©s, caches, transit entre notre datacenter et les FAI, CDN)
La partie cache et transit est primordiale pour nos ma√Ætrise des co√ªts de diffusion, afin d'utiliser le moins possible les services de CDN.
C'est pour cela qu'il doit √™tre rapide de basculer la diffusion vid√©o d'un point vers un autre, en fonction des besoins.
## Architecture
(ins√©rer sch√©ma high-level ici)
## Technos utilis√©es dans la vid√©o
Une grande partie de nos services est d√©velopp√©e en interne gr√¢ce √† des projets OpenSource, mais nous avons recours √† des syst√®mes propri√©taires pour certains aspects tr√®s techniques (encodage/transcodage, packaging et g√©n√©ration √† la vol√©e des diff√©rents formats)
### Dans la partie m√©tadonn√©es
Le service MOVE ("Outil Vid√©o Multi-Ecrans"), qui est notre backoffice de commande de replays, de d√©coupe d'extraits et de livraison aux partenaires, est √©crit en PHP/Symfony avec du MySQL derri√®re (oui, il vit depuis quelques ann√©es).
Le service de r√©ferentiel vid√©o, qui regroupe toutes les m√©tadonn√©es des vid√©os, a une API √©crite en NodeJS et une autre en Go. Son stockage primaire est une base Postgresql (avec utilisation de champs JSON)
Le syst√®me de notifications de changement de m√©tadonn√©es est architectur√© autour de RabbitMQ.
Les services de mises √† jour des m√©tadonn√©es vid√©o cot√© publicit√© sont √©crits en Go.
Le service de m√©tadonn√©es vid√©o (mediainfo) appel√© par les players est √©crit en Go.
Au niveau DRM, nous avons le service Widevine et le service Fairplay qui sont √©crits en Go, et le service Playready qui est √©crit en C# (car SDK .NET)
### Dans la partie vid√©o proprement dite
Le pilotage des transcodages est effectu√© par un outil (videoworkflow), √©crit en Go et s'appuyant sur RabbitMQ.
Les transcodeurs sont des Elemental Server. Ce sont des serveurs propri√©taires avec des GPU pour acc√©l√©rer les traitements. Ils disposent d'un backoffice web et d'une API REST, par lesquels on peut cr√©er des profils d'encodage et soumettre des jobs.
Le syst√®me de g√©n√©ration √† la demande des diff√©rents formats vid√©o, avec gestion des DRM et des sous-titres, est √©galement propri√©taire, de chez Unified Streaming.
Nos caches sont bas√©s sur l'excellent serveur Web nginx, avec des serveurs physiques gav√©s de RAM et de disque.

# Le cloud et le devops

Nos services sont d√©ploy√©s sur des clusters kubernetes. Nous utilisons le cloud [AWS](https://aws.amazon.com/) pour h√©berger ces clusters (service [EKS](https://aws.amazon.com/eks/)).

## L'infra as code

TODO AWS + Terraform

## L'auto scaling

TODO fonctionnement + explication linkerD

## La CI/CD

TODO explication jenkins + tests non reg / replayer + spinnaker
