<html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=icon href=/images/favicon-96x96.png><link rel=stylesheet type=text/css href="//fonts.googleapis.com/css?family=Open+Sans"><link rel=stylesheet href=/scss/global.min.ae6b060df7a6cc9d3fc742acd2d5dad1ccc9ba6004fb2b70d30d45768e08f41b.css><link rel=stylesheet href=/css/prism.css><link href="https://fonts.googleapis.com/css?family=Merriweather&display=swap" rel=stylesheet><title>Déploiement d'un LLM à l'échelle avec TGI | Blog technique e-TF1</title><meta name=description content="Découvrez comment nous réalisons l'inférence de nos LLMs en production."><meta name=robots content="index, follow"><meta name=google-site-verification content="8mELmXNpnm-09GtDb3i1kq4Jfq_iR94-yqgo5wN9CdY"><meta property="og:title" content="Déploiement d'un LLM à l'échelle avec TGI | Blog technique e-TF1"><meta property="og:site_name" content="Blog technique e-TF1"><meta property="og:description" content="Découvrez comment nous réalisons l'inférence de nos LLMs en production."><meta property="og:url" content="https://tech.tf1.fr/post/2024/ia/inference-llm-tgi/"><meta property="og:type" content="website"><meta property="og:locale" content="fr_FR"><meta property="og:image" content="https://tech.tf1.fr/post/2024/ia/inference-llm-tgi/images/hero.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Déploiement d'un LLM à l'échelle avec TGI | Blog technique e-TF1"><link rel=canonical href=https://tech.tf1.fr/post/2024/ia/inference-llm-tgi/><meta name=twitter:description content="Découvrez comment nous réalisons l'inférence de nos LLMs en production."><meta name=twitter:image content="https://tech.tf1.fr/post/2024/ia/inference-llm-tgi/images/hero.jpg"><meta property="article:published_time" content="2024-06-26T09:00:00+00:00"><meta property="article:updated_time" content="2024-06-26T09:00:00+00:00"><meta property="keywords" content="mytf1, tf1, tf1+, streaming, player, video, go, golang, react, js, javascript, css, android, ios, kotlin, swift, nginx, drm, widevine, elemental, aws, mongodb, kafka"><link rel=alternate type=application/rss+xml title="Blog technique e-TF1" href=https://tech.tf1.fr/index.xml></head><body class=line-numbers><script src=/js/initColors.js></script><div class=layout-styled><section class=section><div class=nav-container><a class=logo-link href=/><img src=/images/logo-tf1plus-white.svg alt=logo id=logo-desktop>
<span class=header-hidden>Navigate back to the homepage</span></a><div class=nav-controls><button id=copyButton class=icon-wrapper><svg class="icon-image" width="24" height="20" viewBox="0 0 24 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path fillRule="evenodd" clipRule="evenodd" d="M2 5C2 3.34328 3.34328 2 5 2h9c1.6567.0 3 1.34328 3 3V9c0 1.6567-1.3433 3-3 3H10C9.44771 12 9 12.4477 9 13S9.44771 14 10 14h4c2.7613.0 5-2.2387 5-5V5c0-2.76128-2.2387-5-5-5H5C2.23872.0.0 2.23872.0 5V9c0 1.4938.656313 2.8361 1.6935 3.7509C2.10768 13.1163 2.73961 13.0767 3.10494 12.6625 3.47028 12.2483 3.43068 11.6164 3.0165 11.2511 2.39169 10.6999 2 9.89621 2 9V5zm5 6c0-1.65672 1.34328-3 3-3h4C14.5523 8 15 7.55228 15 7S14.5523 6 14 6H10C7.23872 6 5 8.23872 5 11v4c0 2.7613 2.23872 5 5 5h9c2.7613.0 5-2.2387 5-5V11C24 9.50621 23.3437 8.16393 22.3065 7.24906 21.8923 6.88372 21.2604 6.92332 20.8951 7.3375 20.5297 7.75168 20.5693 8.38361 20.9835 8.74894 21.6083 9.30007 22 10.1038 22 11v4c0 1.6567-1.3433 3-3 3H10c-1.65672.0-3-1.3433-3-3V11z" fill="#000"/></svg><div id=toolTip class=tool-tip>copied</div><input id=copyText style=opacity:0 type=text class=tool-tip></button>
<button id=themeColorButton class=icon-wrapper><div id=sunRays class=sun-rays></div><div id=moonOrSun class=moon-or-sun></div><div id=moonMask class=moon-mask></div></button></div></div></section><script src=/js/toggleLogos.js></script>
<script src=/js/toggleColors.js></script>
<script src=/js/copyUrl.js></script><section class="section narrow"><section id=articleHero class="section narrow"><div class=article-hero><header class=article-header><h1 class=article-hero-heading>Déploiement d'un LLM à l'échelle avec TGI</h1><div class=article-hero-subtitle><div class=article-meta><a href=/authors/rpinsonneau/ class=article-author-link><div class=article-author-avatar><img src=/authors/rpinsonneau/avatar.jpg></div><strong>Rémy Pinsonneau</strong>
<span class=hide-on-mobile>,&nbsp;</span></a>
<script src=/js/collapseAuthors.js></script>
Publié le 26 juin 2024 • 9 minutes</div></div></header><div class=article-hero-image id=ArticleImage__Hero><img src=/post/2024/ia/inference-llm-tgi/images/hero.jpg></div></div></section><aside id=progressBar class=aside-container><div class=aside-align><div><div class=overlap-container></div></div></div><div class=progress-container tabindex={-1}><div class=track-line aria-hidden=true><div id=progressIndicator class=progress-line></div></div></div></aside><article id=articleContent class=post-content style=position:relative><h2 id=linférence-dun-large-language-model>L&rsquo;inférence d&rsquo;un Large Language Model</h2><p>Les LLMs (<em>Large Language Model</em>) sont de plus en plus adoptés en entreprise, leur coût, leur mise à l&rsquo;échelle en production ou la confidentialité des données peuvent être de véritables défis.</p><p>La solution la plus simple pour réaliser l&rsquo;inférence d&rsquo;un modèle consiste à payer une solution clé en main, telle que :</p><ul><li><a href=https://platform.openai.com/docs>openAI (chatGPT)</a></li><li><a href=https://ai.google.dev>Google (Gemini)</a></li><li><a href=https://docs.anthropic.com/en/api>Anthropic (Claude)</a></li></ul><p>Certains services, comme <a href=https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html>AWS Bedrock</a> permettent de déployer différents modèles.</p><p><img src=images/aws-bedrock.png#darkmode alt="AWS Bedrock" title="Modèles d'AWS Bedrock"></p><p>Souvent, le prix se fera en fonction du nombre de tokens (input + output), ce qui n&rsquo;aide pas à se projeter sur le coût réel de la solution. La confidentialité des données est également un point de vigilance d&rsquo;un point de vue RGPD. En utilisant une solution SaaS, nous entrons dans une logique de &ldquo;vendor lock-in&rdquo;, et nous perdons également la maîtrise sur les potentielles mises à jour des modèles.</p><p>L&rsquo;autre possibilité est de déployer soi-même un modèle Open Source :</p><ul><li><a href=https://mistral.ai/technology/#models>Mistral</a></li><li><a href=https://llama.meta.com/llama3/>LLama</a></li><li><a href=https://azure.microsoft.com/en-us/products/phi-3>Phi3</a></li></ul><p>Chaque modèle vient avec sa façon de réaliser l&rsquo;inférence, il existe cependant des outils qui permettent d&rsquo;abstraire le déploiement de ces modèles sans distinction:</p><ul><li><a href=https://ollama.com/>Ollama</a></li><li><a href=https://github.com/ggerganov/llama.cpp>llama.cpp</a></li><li><a href=https://docs.vllm.ai/en/stable/>vLLM</a></li><li><a href=https://github.com/huggingface/text-generation-inference>TGI (Text Generation Inference)</a></li><li><a href=https://github.com/huggingface/text-embeddings-inference>TEI (Text Embeddings Inference)</a></li></ul><h3 id=hugging-face--tgi>Hugging Face & TGI</h3><p>À eTF1, nous utilisons les solutions d&rsquo;<a href=https://huggingface.co/>hugging face</a> sur nos environnements AWS. <a href=https://ollama.com/>Ollama</a> est également utilisé sur les postes de développement pour du prototypage.</p><p><a href=https://huggingface.co/>Hugging face</a> est un hub, qui permet de partager des datasets et des modèles à la communauté. C&rsquo;est aussi un ensemble de bibliothèques qui permettent l&rsquo;entraînement et l&rsquo;inférence de ces modèles.</p><p><img src=images/huggingface.png#darkmode alt="Hugging Face" title="Hugging Face Hub"></p><p><a href=https://github.com/huggingface/text-generation-inference>TGI</a> (<em>Text Generation Inference</em>) permet de déployer facilement un modèle, fourni sous forme d&rsquo;image Docker, <a href=https://github.com/huggingface/text-generation-inference>TGI</a> peut être facilement déployé sur une infrastructure existante.</p><p>L&rsquo;inférence d&rsquo;un LLM nécessite cependant l&rsquo;utilisation d&rsquo;une configuration matérielle (<em>hardware</em>) spécifique, <a href=https://github.com/huggingface/text-generation-inference>TGI</a> permet l&rsquo;inférence sur différents types de matériels :</p><ul><li>Une carte graphique NVIDIA, AMD ou Intel : sur AWS, à Paris (zone eu-west-3), les instances EC2 de type <a href=https://aws.amazon.com/fr/ec2/instance-types/g4/>G4dn</a> sont disponibles avec des cartes NVIDIA T4 avec 16GB de VRAM pour ~0,6$ de l&rsquo;heure pour un g4dn.xlarge.</li><li>Une carte d&rsquo;accélération spécifique Inferentia, Gaudi, TPU : sur AWS, les instances EC2 de type <a href=https://aws.amazon.com/fr/ec2/instance-types/inf2/>Inf2</a> sont disponibles avec des cartes Inferentia2 avec 2x16GB de mémoire pour ~1$ de l&rsquo;heure pour inf2.xlarge.</li></ul><p>Pour démarrer l&rsquo;inférence du modèle, il suffit de démarrer le conteneur TGI. Sur AWS, nous déployons ces conteneurs dans un cluster <a href=https://aws.amazon.com/fr/eks/>EKS</a>. Nous avons une configuration spécifique de <a href=https://karpenter.sh/>Karpenter</a> pour provisionner des instances EC2 de type <a href=https://aws.amazon.com/fr/ec2/instance-types/g4/>G4dn</a> ou <a href=https://aws.amazon.com/fr/ec2/instance-types/inf2/>Inf2</a> sur ces déploiements.</p><p>Exemple de commande docker :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run --rm -p 8080:80                                 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>/data:/data                                <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --gpus all                                          <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HUGGING_FACE_HUB_TOKEN<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>HF_TOKEN<span style=color:#e6db74>}</span>               <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       ghcr.io/huggingface/text-generation-inference:2.0.4 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --model-id<span style=color:#f92672>=</span>mistralai/Mistral-7B-Instruct-v0.3
</span></span></code></pre></div><p>Pour tester Meta Llama 3 au lieu de Mistral il suffit de changer l&rsquo;argument <code>--model-id</code>. Les identifiants des modèles sont indiqués sur le hub Hugging Face.</p><p>Au démarrage, TGI va télécharger dans un volume (<code>/data</code>) le modèle spécifié si celui-ci n&rsquo;est pas déjà présent.</p><h3 id=quantization>Quantization</h3><p>En plus d&rsquo;une configuration matérielle spécifique, il faudra être vigilant à la quantité de mémoire nécessaire pour exécuter le modèle. Nous parlons bien ici de mémoire GPU (ou mémoire de la carte d&rsquo;accelération) et non de RAM.</p><p>Ci-dessous les prérequis pour exécuter les différentes variantes de Mistral :</p><table><thead><tr><th>Name</th><th>Number of parameters</th><th>Number of active parameters</th><th>Min. GPU RAM for inference (GB)</th></tr></thead><tbody><tr><td>Mistral-7B-v0.3</td><td>7.3B</td><td>7.3B</td><td>16</td></tr><tr><td>Mixtral-8x7B-v0.1</td><td>46.7B</td><td>12.9B</td><td>100</td></tr><tr><td>Mixtral-8x22B-v0.3</td><td>140.6B</td><td>39.1B</td><td>300</td></tr><tr><td>Codestral-22B-v0.1</td><td>22.2B</td><td>22.2B</td><td>60</td></tr></tbody></table><p>Heureusement, il existe plusieurs techniques pour rendre l&rsquo;inférence possible sur du matériel relativement modeste ou grand public (<em>commodity hardware</em>) : le sharding et la <a href=https://huggingface.co/docs/optimum/concept_guides/quantization>quantization</a>.</p><p>Lorsque TGI a terminé le téléchargement d&rsquo;un modèle dans son volume, il commence ensuite le chargement de celui-ci dans la mémoire de la carte d&rsquo;accélération.</p><p><img src=images/nvidia-smi.png#darkmode alt=nvidia-smi title=nvidia-smi></p><p>Le modèle est souvent sauvegardé sous forme de fichier binaire, par exemple <a href=https://pytorch.org/tutorials/beginner/saving_loading_models.html>pytorch</a> ou <a href=https://github.com/huggingface/safetensors>safetensors</a>.</p><p><img src=images/mistral-files.png#darkmode alt="Mistral model files" title="Mistral model files"></p><p>Ces fichiers volumineux contiennent les fameux paramètres du modèle. En réalité il s&rsquo;agit de matrices contenant les différents paramètres sous forme de nombres flottants (<code>float</code>). Nous pouvons facilement trouver la précision des nombres flottants d&rsquo;un modèle en consultant le <a href=https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/blob/main/config.json>fichier de configuration</a> de Hugging Face du modèle. Pour Mistral et Llama 3, l&rsquo;attribut <code>torch_dtype</code> a comme valeur <a href=https://pytorch.org/docs/stable/tensors.html>&ldquo;bfloat16&rdquo;</a>.</p><p>En réduisant la précision des nombres flottants d&rsquo;un modèle, nous réduisons aussi sa taille. Mistral-7B-v0.3 nécessite 16GB de VRAM pour une inférence avec 16 bits de précision, la quantité de mémoire nécessaire pour une précision de 8 bits ou 4 bits sera deux ou quatre fois moins élevée. Cette diminution implique une réduction de la performance du modèle, cependant celle-ci reste en général limitée à l&rsquo;usage.</p><p>Sous macOS, avec <a href=https://ollama.com/>Ollama</a>, les modèles sont en général en 4 bits, et s&rsquo;exécutent dans la mémoire unifiée. Les NVIDIA supportent la quantization, tandis que sur Inferentia ce n&rsquo;est pas le cas. Il faut donc vérifier les contraintes en fonction de la configuration matérielle. De plus, il existe plusieurs implémentations pour <em>quantizer</em> un modèle, certaines nécessitent une variante du modèle d&rsquo;origine :</p><ul><li><a href=https://github.com/TimDettmers/bitsandbytes>bitsandbytes</a> : fonctionne avec tous les modèles mais peut être plus lent</li><li><a href=https://github.com/NetEase-FuXi/EETQ>EETQ</a>: 8 bits, fonctionne avec tous les modèles</li><li><a href=https://github.com/casper-hansen/AutoAWQ>AWQ</a> : 4 bits, nécessite un modèle <a href="https://hf.co/models?search=awq">spécifique</a></li></ul><p>Par exemple pour une quantization 8 bits sur T4 avec eetq :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run --rm -p 8080:80                                 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>/data:/data                                <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --gpus all                                          <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HUGGING_FACE_HUB_TOKEN<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>HF_TOKEN<span style=color:#e6db74>}</span>               <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       ghcr.io/huggingface/text-generation-inference:2.0.4 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --model-id<span style=color:#f92672>=</span>mistralai/Mistral-7B-Instruct-v0.3       <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --quantize eetq
</span></span></code></pre></div><h3 id=sharding>Sharding</h3><p>Sur les instances inferentia2 d&rsquo;AWS, 2 coeurs (<em>cores</em>) sont disponibles sur les inf2.xlarge. On a donc 2x16GB, pour exploiter pleinement les 32GB disponibles il faut scinder le modèle en deux.</p><p><img src=images/tgi-sharding.png#darkmode alt="TGI architecture" title="TGI architecture"></p><p>TGI supporte nativement le <em>sharding</em> et va donc exploiter les deux coeurs disponibles.</p><p>Exemple de commande docker pour mistral sur inferentia2 inf2.xlarge :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run --rm -p 8080:80                            <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>/data:/data                           <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --device<span style=color:#f92672>=</span>/dev/neuron0                          <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --shm-size 1g                                  <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HF_TOKEN<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>HF_TOKEN<span style=color:#e6db74>}</span>                        <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HF_BATCH_SIZE<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>                             <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HF_SEQUENCE_LENGTH<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>                     <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HF_AUTO_CAST_TYPE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;fp16&#34;</span>                    <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HF_NUM_CORES<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>                              <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       ghcr.io/huggingface/neuronx-tgi:latest         <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --model-id<span style=color:#f92672>=</span>mistralai/Mistral-7B-Instruct-v0.3  <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --max-batch-size <span style=color:#ae81ff>1</span>                             <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --max-total-tokens <span style=color:#ae81ff>4096</span>
</span></span></code></pre></div><p>Pour exécuter TGI sur inferentia2, il est nécessaire d&rsquo;utiliser une image Docker <a href=https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference>spécifique</a>. D&rsquo;autre part, le modèle doit être recompilé pour s&rsquo;exécuter sur inferentia à l&rsquo;aide d&rsquo;un outil : <a href=https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/index.html>Neuron Compiler</a>. Hugging Face dispose d&rsquo;un cache avec les modèles précompilés, par exemple pour Mistral nous trouvons la liste <a href=https://huggingface.co/aws-neuron/optimum-neuron-cache/blob/main/inference-cache-config/mistral.json>ici</a>. Les paramètres <code>HF_BATCH_SIZE</code>, <code>HF_SEQUENCE_LENGTH</code>, <code>HF_AUTO_CAST_TYPE</code> et <code>HF_NUM_CORES</code> doivent correspondre aux paramètres utilisés lors de la compilation.</p><p><img src=images/neuron-top.png#darkmode alt="Neuron top" title="Neuron top"></p><h3 id=batching>Batching</h3><p>Pour augmenter les performances d&rsquo;inférence en production, il est primordial d&rsquo;utiliser la technique de batching.
Lorsqu&rsquo;un prompt est soumis à un LLM, il est transformé en tokens, la première itération permet de déterminer le premier token de la réponse. La seconde itération prend en entrée le prompt et le premier token généré de la réponse afin de générer le deuxième token.
Le temps d&rsquo;inférence est principalement lié au temps nécessaire pour charger les données dans la mémoire du GPU, il est donc indispensable de faire travailler le GPU sur plusieurs prompts en même temps : les itérations des différents prompts sont <em>batchées</em> pour minimiser le nombre de chargements en mémoire.</p><p>TGI supporte nativement le <em>continuous batching</em>, pour affiner le comportement des batch il est possible de jouer sur les paramètres suivants :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run --rm -p 8080:80                                 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>/data:/data                                <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --gpus all                                          <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --shm-size 1g                                       <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HUGGING_FACE_HUB_TOKEN<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>HF_TOKEN<span style=color:#e6db74>}</span>               <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       ghcr.io/huggingface/text-generation-inference:2.0.4 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --model-id<span style=color:#f92672>=</span>mistralai/Mistral-7B-Instruct-v0.3       <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --quantize eetq                                     <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --max-input-length <span style=color:#ae81ff>1984</span>                             <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --max-total-tokens <span style=color:#ae81ff>2048</span>
</span></span></code></pre></div><p>L&rsquo;option <a href=https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher#maxtotaltokens><code>--max-total-tokens</code></a> est structurante. Celle-ci doit être déterminée en fonction du nombre de tokens en input et du nombre de tokens attendus en output. Plus sa valeur sera basse, plus un batch pourra contenir d&rsquo;itérations.</p><p>A noter, sur les instances inferentia, le batching est statique et déterminé à la compilation contrairement à d&rsquo;autres configurations matérielles où le nombre d&rsquo;itérations dans un batch est dynamique.</p><h3 id=guidance--json>Guidance / JSON</h3><p>Une des difficultés récurrente de l&rsquo;utilisation d&rsquo;un LLM est son intégration avec des briques logicielles. Un LLM est conçu pour répondre en langage naturel. Pour obtenir un retour en JSON il peut être fastidueux de décrire un retour précis dans le prompt, qui de toute façon ne serait pas toujours respecté. Pour contrer cela il est possible de préciser un JSON Schema lors de l&rsquo;appel à TGI.
Le schema est alors inclus dans les batchs et va pondérer les poids sur les tokens de sortie afin de respecter le schema précisé.</p><p>Cette fonctionnalité est documentée <a href=https://huggingface.co/docs/text-generation-inference/conceptual/guidance>ici</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl <span style=color:#e6db74>&#39;http://localhost:8080/generate&#39;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#39;Accept: application.json&#39;</span>       <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -H <span style=color:#e6db74>&#39;Content-Type: application/json&#39;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --data-raw <span style=color:#e6db74>$&#39;{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;inputs&#34;:&#34;vous devrez classifier le message suivant selon son émotion: \\&#34;le plus beau jour de ma vie !\\&#34;&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;parameters&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;temperature&#34;: 0.5,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;max_new_tokens&#34;: 100,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;repetition_penalty&#34;: 1.03,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;grammar&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;type&#34;: &#34;json&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            &#34;value&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                &#34;properties&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    &#34;emotion&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                        &#34;type&#34;: &#34;string&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                        &#34;description&#34;: &#34;émotion associée au message&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                        &#34;enum&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                            &#34;tristesse&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                            &#34;colère&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                            &#34;joie&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                            &#34;peur&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                        ]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    },
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    &#34;explanation&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                        &#34;type&#34;: &#34;string&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                        &#34;description&#34;: &#34;un explication en français et concise de votre classification&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                },
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                &#34;required&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    &#34;emotion&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                    &#34;explanation&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>                ]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    }
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}&#39;</span> |jq <span style=color:#e6db74>&#39;.generated_text | fromjson&#39;</span>
</span></span></code></pre></div><p>Le retour du LLM :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;emotion&#34;</span>: <span style=color:#e6db74>&#34;joie&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;explanation&#34;</span>: <span style=color:#e6db74>&#34;Le message contient l&#39;expression &#39;le plus beau jour de ma vie&#39;, qui est généralement associée à une émotion de joie ou de bonheur.&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=embeddings>Embeddings</h3><p>Pour mettre en place des techniques de RAG (<em>Retrieval Augmented Generation</em>) il est possible d&rsquo;utiliser un autre outil d&rsquo;Hugging Face : <a href=https://github.com/huggingface/text-embeddings-inference>TEI</a> (<em>Text Embeddings Inference</em>).
AWS supporte un certain nombre de <a href=https://aws.amazon.com/what-is/vector-databases/>bases de données vectorielles</a> qui permettent de stocker les embeddings de vos documents.</p><p>Exemple de commande docker pour démarrer TEI sur une instance T4 (architecture turing)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run --rm -p 8081:80                                      <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>/data:/data                                     <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --gpus all                                               <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --shm-size 1g                                            <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       -e HF_TOKEN<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>HF_TOKEN<span style=color:#e6db74>}</span>                                  <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       ghcr.io/huggingface/text-embeddings-inference:turing-1.2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --model-id BAAI/bge-m3                                   <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>       --port <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><h2 id=conclusion>Conclusion</h2><p>TGI permet de déployer simplement un LLM Open Source et fait abstraction des spécificités des modèles.
Le support de différentes configurations matérielles apporte plus de souplesse selon les besoins.
Les fonctionnalités de <em>sharding</em>, <em>batching</em> et <em>quantization</em> permettent d&rsquo;optimiser les performances tout en rendant possible l&rsquo;inférence sur du materiel grand public. Les instances EC2 équipées d&rsquo;une NVIDIA T4 ne consomment que 70W ce qui est plutôt raisonnable pour déployer un LLM.</p><p>La possibilité de guider le modèle avec un JSON Schema est un vrai plus pour l&rsquo;automatisation de tâches.</p><p>TGI répond à nos besoin actuels, vLLM serait intérressant à explorer pour des besoins d&rsquo;inférence avec des enjeux plus importants en scalabilité.</p><p>L&rsquo;inférence d&rsquo;un LLM est coûteuse, avec l&rsquo;engouement autour de leur usage, il est important de bien définir les cas d&rsquo;utilisation dans lesquels ils sont réellement pertinents. Par ailleur il est crucial de bien sizer les instances pour éviter un gaspillage des resources pour un usage plus responsable de cette technologie.</p></article><section id=articleNext class="section nartrow"><h3 class=footer-next-heading>Plus d'articles</h3><div class=footer-spacer></div><div class=next-articles-grid numberofarticles={numberOfArticles}><div class=post-row><a href=/post/2023/pub/streaming-fast-ssai/ class=article-link id=article-link-bigger><div><div class=image-container><img src=/post/2023/pub/streaming-fast-ssai/images/hero.jpg class=article-image></div><div><h2 class=article-title>Gestion du Server-Side Ad Insertion (SSAI) sur nos chaînes FAST</h2><p class=article-excerpt>Découvrez comment nous avons mis en place nos chaînes FAST et notre solution SSAI pour y insérer la pub.</p><div class=article-metadata>6 novembre 2023 • 13 minutes</div></div></div></a><a href=/post/2023/architecture/workflow-video-avec-temporal/ class=article-link><div><div class=image-container><img src=/post/2023/architecture/workflow-video-avec-temporal/images/hero.jpg class=article-image></div><div><h2 class=article-title>Workflow d'encodage et delivery vidéo avec Temporal</h2><p class=article-excerpt>Découvrez comment nous avons mis en place notre nouveau workflow d'encodage et de delivery vidéo avec Temporal.</p><div class=article-metadata>23 juillet 2023 • 12 minutes</div></div></div></a></div></div></section></section><script src=/js/progressBar.js></script><div class=footer-gradient></div><div class="section narrow"><div class=footer-hr></div><div class=footer-container><div class=footer-text><img src=/images/logo_tf1lg.png class=footer-logo>TF1 Copyright © 2025 e-TF1</div><div class=social-icon-outer><div class=social-icon-container><a href=https://www.welcometothejungle.com/fr/companies/groupe-tf1><svg class="social-icon-image" viewBox="7 7 26 26" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" alt="Rejoingez nous !" title="Rejoignez nous !"><path d="M31.347322 11.295661C30.1466441 9.31125424 26.6585085 8.44345763 24.1202034 11.0048136 23.9785085 11.1532881 23.7913898 11.6054915 24.3554576 12.2149831c1.181017 1.258305 7.340339 7.7179661 5.0616949 13.36-1.1071186 2.7430508-4.2555932 2.2705084-5.438644-2.2738984C23.4991864 21.4590508 22.7595254 17.6326102 23.468678 13.0393898 23.468678 13.0393898 23.5818983 12.6082034 23.3995254 12.4705763 23.2734237 12.3743051 23.0707119 12.4143051 22.8103729 12.8312542 22.7452881 12.936339 22.7588475 12.9146441 22.6964746 13.0265085L22.6924068 13.0285424c-1.7457627 2.9444068-3.5220339 6.4352542-4.3579661 7.7444068C18.7425763 18.7980339 18.2252881 16.0814237 17.4185085 13.8075254L17.2666441 13.396678C16.5215593 11.4515932 15.6259661 10.1390508 15.3541017 9.84142373 15.3215593 9.80549153 15.2408814 9.73837288 14.9744407 9.73837288H7.42461017c-.528135590000001.0-.4820339.45220342-.31525424.615593219999999 2.00474576 2.3857627 7.84610167 11.3294915 5.13762717 20.1064407C12.0381695 31.1390508 12.3663051 31.4949831 12.9066441 30.7946441c2.3742373-3.0772882 5.32-8.3227119 7.4542373-12.2542373C19.8903729 19.9037966 19.4761356 21.4217627 19.2544407 23.0122712c-.4677966 3.3572881.5247457 5.8827119 1.2508474 7.2081356C20.8768136 30.8990508 21.3696949 31.3139661 22.6456271 31.1241356c6.5071187-.9694915 8.6772882-6.7016949 9.4786441-9.6088136C33.5852881 16.215661 32.5154576 13.2258305 31.347322 11.295661" id="monogram" fill="#000"/></svg></a><span class=hidden>https://www.welcometothejungle.com/fr/companies/groupe-tf1</span>
<a href=https://www.linkedin.com/company/e-tf1/><svg class="social-icon-image" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path fillRule="evenodd" clipRule="evenodd" d="M3.59615 13.125H.871552V4.36523H3.59615V13.125zM2.24847 3.16406c-.42969.0-.80078-.15625-1.11328-.46875-.312498-.3125-.468747-.6836-.468747-1.11328.0-.42969.156249-.800782.468747-1.113281C1.44769.156249 1.81878.0 2.24847.0s.80078.156249 1.11328.468749c.3125.312499.46875.683591.46875 1.113281.0.42968-.15625.80078-.46875 1.11328s-.68359.46875-1.11328.46875zM13.7915 13.125H11.0669V8.84765C11.0669 8.14452 11.0083 7.63671 10.8911 7.32421c-.2148-.52734-.6348-.79101-1.25976-.79101-.625.0-1.06445.23437-1.31836.70312C8.11767 7.58788 8.02001 8.10546 8.02001 8.78905V13.125H5.32471V4.36523H7.93212V5.5664H7.96142C8.15673 5.17578 8.46923 4.85351 8.89892 4.59961c.46875-.3125 1.01562-.46875 1.64058-.46875 1.2696.0 2.1582.40039 2.666 1.20117C13.5962 5.97656 13.7915 6.97265 13.7915 8.3203V13.125z" fill="#73737d"/></svg></a><span class=hidden>https://www.linkedin.com/company/e-tf1/</span>
<a href=https://github.com/etf1><svg class="social-icon-image" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path fillRule="evenodd" clipRule="evenodd" d="M7 0C3.1325.0.0 3.21173.0 7.17706.0 10.3529 2.00375 13.0353 4.78625 13.9863 5.13625 14.0491 5.2675 13.8338 5.2675 13.6454 5.2675 13.4749 5.25875 12.9097 5.25875 12.3087 3.5 12.6406 3.045 11.8691 2.905 11.4653 2.82625 11.259 2.485 10.622 2.1875 10.4516 1.9425 10.317 1.5925 9.98508 2.17875 9.97611 2.73 9.96714 3.12375 10.4964 3.255 10.7118c.63 1.0855 1.63625.7805 2.03875.5921C5.355 10.8374 5.53875 10.5234 5.74 10.3439 4.1825 10.1645 2.555 9.54549 2.555 6.80026c0-.7805.27125-1.42644.7175-1.92883C3.2025 4.692 2.9575 3.95635 3.3425 2.96951c0 0 .58625-.1884 1.925.73565.56-.16149 1.155-.24223 1.75-.24223s1.19.08074 1.75.24223c1.3388-.93302 1.925-.73565 1.925-.73565C11.0775 3.95635 10.8325 4.692 10.7625 4.87143 11.2087 5.37382 11.48 6.01079 11.48 6.80026c0 2.7542-1.63625 3.36424-3.19375 3.54364C8.54 10.5682 8.75875 10.9988 8.75875 11.6717 8.75875 12.6316 8.75 13.4032 8.75 13.6454 8.75 13.8338 8.88125 14.0581 9.23125 13.9863 11.9963 13.0353 14 10.3439 14 7.17706 14 3.21173 10.8675.0 7 0z" fill="#73737d"/></svg></a><span class=hidden>https://github.com/etf1</span>
<a href=https://tech.tf1.fr/index.xml><svg class="social-icon-image" style="width:1em;height:1em;vertical-align:middle;fill:currentColor;overflow:hidden" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M329.142857 768q0 45.714286-32 77.714286t-77.714286 32-77.714285-32-32-77.714286 32-77.714286 77.714285-32 77.714286 32 32 77.714286zm292.571429 70.285714q1.142857 16-9.714286 27.428572-10.285714 12-26.857143 12H508q-14.285714.0-24.571429-9.428572T472 844.571429q-12.571429-130.857143-105.428571-223.714286T142.857143 515.428571Q128.571429 514.285714 119.142857 504T109.714286 479.428571V402.285714q0-16.571429 12-26.857143 9.714286-9.714286 24.571428-9.714285h2.857143q91.428571 7.428571 174.857143 46T472 515.428571q65.142857 64.571429 103.714286 148t46 174.857143zm292.571428 1.142857Q915.428571 854.857142 904 866.285714q-10.285714 11.428571-26.285714 11.428572H796q-14.857143.0-25.428571-10t-11.142858-24.285715Q752.571428 720.571428 701.714286 610T569.428571 418t-192-132.285714T144 227.428571q-14.285714-.571429-24.285714-11.142857t-10-24.857143V109.714286q0-16 11.428571-26.285715 10.285714-10.285714 25.142857-10.285714H148q149.714286 7.428571 286.571429 68.571429t243.142857 168q106.857143 106.285714 168 243.142857t68.571428 286.571428z"/></svg></a><span class=hidden>https://tech.tf1.fr/index.xml</span></div></div></div></div></div><script src=/js/prism.js></script></body></html>