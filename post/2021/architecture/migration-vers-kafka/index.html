<html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=icon href=/images/favicon-96x96.png><link rel=stylesheet type=text/css href="//fonts.googleapis.com/css?family=Open+Sans"><link rel=stylesheet href=/scss/global.min.ae6b060df7a6cc9d3fc742acd2d5dad1ccc9ba6004fb2b70d30d45768e08f41b.css><link rel=stylesheet href=/css/prism.css><link href="https://fonts.googleapis.com/css?family=Merriweather&display=swap" rel=stylesheet><title>Migration du backend MYTF1 vers Kafka | Blog technique e-TF1</title><meta name=description content="Venez découvrir pourquoi et comment nous avons mis en oeuvre Kafka sur MYTF1"><meta name=robots content="index, follow"><meta name=google-site-verification content="8mELmXNpnm-09GtDb3i1kq4Jfq_iR94-yqgo5wN9CdY"><meta property="og:title" content="Migration du backend MYTF1 vers Kafka | Blog technique e-TF1"><meta property="og:site_name" content="Blog technique e-TF1"><meta property="og:description" content="Venez découvrir pourquoi et comment nous avons mis en oeuvre Kafka sur MYTF1"><meta property="og:url" content="https://tech.tf1.fr/post/2021/architecture/migration-vers-kafka/"><meta property="og:type" content="website"><meta property="og:locale" content="fr_FR"><meta property="og:image" content="https://tech.tf1.fr/post/2021/architecture/migration-vers-kafka/images/hero.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Migration du backend MYTF1 vers Kafka | Blog technique e-TF1"><link rel=canonical href=https://tech.tf1.fr/post/2021/architecture/migration-vers-kafka/><meta name=twitter:description content="Venez découvrir pourquoi et comment nous avons mis en oeuvre Kafka sur MYTF1"><meta name=twitter:image content="https://tech.tf1.fr/post/2021/architecture/migration-vers-kafka/images/hero.jpeg"><meta property="article:published_time" content="2021-02-19T08:30:00+00:00"><meta property="article:updated_time" content="2021-02-19T08:30:00+00:00"><meta property="keywords" content="mytf1, tf1, streaming, player, video, go, golang, react, js, javascript, css, android, ios, kotlin, swift, nginx, drm, widevine, elemental, aws, mongodb, kafka"><link rel=alternate type=application/rss+xml title="Blog technique e-TF1" href=https://tech.tf1.fr/index.xml></head><body class=line-numbers><script src=/js/initColors.js></script><div class=layout-styled><section class=section><div class=nav-container><a class=logo-link href=/><img src=/images/logo-white.svg alt=logo id=logo-desktop>
<span class=header-hidden>Navigate back to the homepage</span></a><div class=nav-controls><button id=copyButton class=icon-wrapper><svg class="icon-image" width="24" height="20" viewBox="0 0 24 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path fillRule="evenodd" clipRule="evenodd" d="M2 5C2 3.34328 3.34328 2 5 2h9c1.6567.0 3 1.34328 3 3V9c0 1.6567-1.3433 3-3 3H10C9.44771 12 9 12.4477 9 13S9.44771 14 10 14h4c2.7613.0 5-2.2387 5-5V5c0-2.76128-2.2387-5-5-5H5C2.23872.0.0 2.23872.0 5V9c0 1.4938.656313 2.8361 1.6935 3.7509C2.10768 13.1163 2.73961 13.0767 3.10494 12.6625 3.47028 12.2483 3.43068 11.6164 3.0165 11.2511 2.39169 10.6999 2 9.89621 2 9V5zm5 6c0-1.65672 1.34328-3 3-3h4C14.5523 8 15 7.55228 15 7S14.5523 6 14 6H10C7.23872 6 5 8.23872 5 11v4c0 2.7613 2.23872 5 5 5h9c2.7613.0 5-2.2387 5-5V11C24 9.50621 23.3437 8.16393 22.3065 7.24906 21.8923 6.88372 21.2604 6.92332 20.8951 7.3375 20.5297 7.75168 20.5693 8.38361 20.9835 8.74894 21.6083 9.30007 22 10.1038 22 11v4c0 1.6567-1.3433 3-3 3H10c-1.65672.0-3-1.3433-3-3V11z" fill="#000"/></svg><div id=toolTip class=tool-tip>copied</div><input id=copyText style=opacity:0 type=text class=tool-tip></button>
<button id=themeColorButton class=icon-wrapper><div id=sunRays class=sun-rays></div><div id=moonOrSun class=moon-or-sun></div><div id=moonMask class=moon-mask></div></button></div></div></section><script src=/js/toggleLogos.js></script><script src=/js/toggleColors.js></script><script src=/js/copyUrl.js></script><section class="section narrow"><section id=articleHero class="section narrow"><div class=article-hero><header class=article-header><h1 class=article-hero-heading>Migration du backend MYTF1 vers Kafka</h1><div class=article-hero-subtitle><div class=article-meta><a href=/authors/vcomposieux/ class=article-author-link><div class=article-author-avatar><img src=/authors/vcomposieux/avatar.jpg></div><strong>Vincent Composieux</strong>
<span class=hide-on-mobile>,&nbsp;</span></a>
<script src=/js/collapseAuthors.js></script>Publié le 19 février 2021 • 11 minutes</div></div></header><div class=article-hero-image id=ArticleImage__Hero><img src=/post/2021/architecture/migration-vers-kafka/images/hero.jpeg></div></div></section><aside id=progressBar class=aside-container><div class=aside-align><div><div class=overlap-container></div></div></div><div class=progress-container tabindex={-1}><div class=track-line aria-hidden=true><div id=progressIndicator class=progress-line></div></div></div></aside><article id=articleContent class=post-content style=position:relative><h2 id=historique>Historique</h2><p>Comme vous avez pu le découvrir sur notre article <a href=/post/2020/architecture/presentation/>Tour d&rsquo;horizon technique</a>, notre stack backend est chargée d&rsquo;exposer les données aux différents fronts.</p><p>Chez MYTF1, la partie contenu vidéo est gérée par une équipe dédiée. Son rôle est de faire le lien entre l&rsquo;antenne (les chaines de télévision) et la plateforme de diffusion de MYTF1.
Pour ce faire, l&rsquo;équipe vidéo récupère différentes données en provenance de l&rsquo;antenne (les fichiers vidéos, les sous-titres, les méta-données, etc.), les convertit dans des formats compatibles avec les différents modes de diffusion proposés par MYTF1 (voir <a href=/post/2020/architecture/video/>l&rsquo;article dédié</a>) puis consolide le tout dans un référentiel interne.</p><p>Le rôle de l&rsquo;équipe backend est d&rsquo;<strong>éditorialiser et transformer les données de ce référentiel</strong> afin qu&rsquo;elles soient exploitables par nos différents fronts digitaux (Web, Mobile et IPTV) via notre gateway GraphQL.</p><p>Jusque là, notre architecture ressemblait à la suivante :</p><p><img src=images/archi-indexeur.svg#darkmode alt="Architecture avec indexation" title="Architecture avec indexation"></p><p>Globalement, après récupération des données fournies par l&rsquo;équipe vidéo, nous les stockons dans la base <a href=https://www.mongodb.org target=_blank>MongoDB</a> dédiée au CMS qui se charge ensuite de les éditorialiser.</p><p>La suite du workflow est déclenchée par notre job nommé &ldquo;Indexeur&rdquo; (exécuté périodiquement, toutes les 15 minutes) qui, comme son nom l&rsquo;indique, va s&rsquo;occuper de l&rsquo;<strong>indexation</strong> mais aussi de la <strong>transformation</strong> des données stockées dans le CMS afin qu&rsquo;elles soient pré-formatées pour les besoins des différents fronts.</p><p>Une fois stockées dans le moteur <a href=https://www.elastic.co/fr/elasticsearch/ target=_blank>Elasticsearch</a>, lorsqu&rsquo;un front fait un appel à notre gateway GraphQL, celle-ci contacte une API gRPC (Catalog API sur le schéma) qui va s&rsquo;occuper de récupérer les données Elasticsearch et de les formatter sous forme d&rsquo;objets Protobuf bien définis à GraphQL. Les résolveurs GraphQL s&rsquo;occupent ensuite d&rsquo;exposer ces objets aux fronts.</p><p>Cette mécanique fonctionne correctement et est même résiliente car en cas de soucis sur notre brique d&rsquo;indexation, elle permet de ne pas impacter les fronts qui disposeront toujours de la dernière version à jour.</p><p>Cependant, principalement pour des besoins d&rsquo;éditorialisation, nous souhaitions supprimer ces 15 minutes d&rsquo;attente entre chaque indexation et avoir un workflow davantage <strong>temps réel</strong> et <a href=https://en.wikipedia.org/wiki/Event-driven_programming target=_blank>event-driven</a>. Ainsi les modifications effectuées par nos équipes édito seront mises en ligne beaucoup plus rapidement (après quelques secondes). Cette approche event-driven nous offrira également de nombreuses possibilités dans le futur, comme réagir aux actions des utilisateurs pour, par exemple, produire un top des vidéos vues en temps réel.</p><p>Nous avons rapidement identifié <a href=https://kafka.apache.org/ target=_blank>Apache Kafka</a> comme un excellent outil nous permettant de gérer nos <strong>événements</strong> mais surtout nous garantir la bonne <strong>persistance</strong> de ceux-ci.</p><h2 id=découpage-en-lots>Découpage en lots</h2><p>Nous avons donc commencé à réfléchir à notre nouvelle architecture et avons décidé d&rsquo;<strong>itérer par lots</strong> afin d&rsquo;avoir des premiers résultats visibles rapidement pour nos utilisateurs.</p><p>En effet, l&rsquo;outil identifié, il nous fallait maintenant voir si Kafka et son éco-système principalement Java pouvait co-éxister avec notre stack technique en <a href=https://www.golang.org target=_blank>Go</a>.</p><p>Nous avons donc voulu <strong>commencer par des choses simples</strong>, produire et consommer des messages depuis un topic Kafka puis ensuite manipuler ces données au travers de streams des topics Kafka.</p><p>Les lots identifiés sont donc les suivants :</p><ul><li>Lot 1 : <strong>Récupérer les données à jour</strong> depuis la source de données principale (la base de données CMS MongoDB) afin de les exposer dans un Redis à GraphQL,</li><li>Lot 2 : Pouvoir <strong>manipuler les données en temps réel</strong> (streams de données) afin de calculer des valeurs à la volée,</li><li>Lot 3 : Migrer les <strong>données utilisateur</strong> dans Kafka afin d&rsquo;avoir des événements utilisateur persistés</li><li>Lot 4 et suivants : Pouvoir <strong>planifier des événements</strong> dans Kafka, de futurs besoins, dont on parlera dans de futurs articles !</li></ul><p>Ces trois premiers lots nous permettaient de mettre en place une grosse partie de la logique événementielle et surtout de confirmer nos choix.</p><h2 id=les-débuts-avec-kafka>Les débuts avec Kafka</h2><p>Nous avons tout d&rsquo;abord identifié les outils à notre disposition capables de répondre à nos premiers besoins, tout en gardant à l&rsquo;esprit que nous souhaitions garder <a href=https://developers.google.com/protocol-buffers/ target=_blank>Protobuf</a> pour la sérialisation des messages dans les topics Kafka et ainsi continuer à <strong>échanger des objets clairement définis</strong>.</p><p><a href=https://www.confluent.io/ target=_blank>Confluent</a> étant aujourd&rsquo;hui un acteur et contributeur majeur sur Apache Kafka, c&rsquo;est assez naturellement que nous avons commencé à utiliser son client <a href=https://github.com/confluentinc/confluent-kafka-go target=_blank>confluent-kafka-go</a>.</p><p>Il nous laisse une totale autonomie au niveau de la sérialisation des messages Kafka et nous avons donc pu exploiter Protobuf sans aucun soucis.</p><p>Nous avons également commencé à étudier le fonctionnement de la suite Confluent, notamment <a href=https://ksqldb.io/ target=_blank>ksqlDB</a> et la <a href=https://docs.confluent.io/platform/current/schema-registry/index.html target=_blank>Schema Registry</a>. Malheureusement, nos essais n&rsquo;ont pas été concluants. De plus ces solutions nous semblaient trop complexes à mettre en place et en inadéquation avec nos besoins.</p><h2 id=lot-1-production-de-données-dans-redis>Lot 1 : Production de données dans Redis</h2><p>Nous avons donc réfléchi à la mise en place de notre premier lot, pour rappel : <strong>récupérer les données</strong> mises à jour en temps réel depuis la base MongoDB, les <strong>transformer en objet Protobuf</strong> et les <strong>projeter dans Redis</strong>.</p><p>Notre architecture évoluerait donc vers ceci :</p><p><img src=images/archi-kafka-lot1.svg#darkmode alt="Architecture Kafka lot1" title="Architecture Kafka lot1"></p><h3 id=récupérer-les-données-mongodb-en-temps-réel>Récupérer les données MongoDB en temps réel</h3><p>Bien que nous connaissions l&rsquo;existence d&rsquo;un <a href=https://www.confluent.io/hub/mongodb/kafka-connect-mongodb target=_blank>connecteur MongoDB</a> fourni par Confluent et écrit en Java, nous disposons d&rsquo;une équipe expérimentée sur Go et MongoDB. Nous avons décidé de nous lancer un petit défi et d&rsquo;écrire notre propre connecteur en Go.</p><p>Nos tests étant concluants sur cette partie, nous l&rsquo;avons écrit et rendu open-source à cette adresse : <a href=https://github.com/etf1/kafka-mongo-watcher target=_blank>https://github.com/etf1/kafka-mongo-watcher</a>.</p><p>En effet, nous utilisons cette application en production depuis presque un an et n&rsquo;avons relevés aucun problème jusqu&rsquo;à maintenant.</p><p>Nous avons besoin d&rsquo;écouter les événements provenant des collections MongoDB suivantes :</p><ul><li>Chaîne</li><li>Programme</li><li>Video</li><li>Personnalité</li></ul><p>Globalement, le principe est d&rsquo;utiliser la fonction de <a href=https://docs.mongodb.com/manual/reference/method/db.collection.watch/ target=_blank>watch</a> de collection fournie par MongoDB et notre application s&rsquo;occupe ensuite de produire les logs d&rsquo;opération (appelés <a href=https://docs.mongodb.com/manual/core/replica-set-oplog/ target=_blank>oplogs</a>) générés dans un topic Kafka pour un traitement ultérieur.</p><p>MongoDB dispose en effet d&rsquo;une mécanique de <a href=https://docs.mongodb.com/manual/changeStreams/ target=_blank>Change Streams</a> permettant de souscrire à tous types d&rsquo;événements pouvant avoir lieu sur la base de données. Dans notre cas, nous écoutons donc les événements des collections.</p><h3 id=produire-des-messages-protobuf-ou-les-projeter>Produire des messages Protobuf ou les projeter</h3><p>Une brique de transformation (Transformer sur le schéma) prend ensuite le relai et récupère donc depuis le topic Kafka les oplogs MongoDB afin de les transformer en objet Protobuf. À ce stade, nous allons alors produire dans un nouveau topic Kafka l&rsquo;objet Protobuf en question :</p><p><img src=images/kafka-transformer-projector.svg#darkmode alt="Kafka transformer / projecteur" title="Kafka transformer / projecteur"></p><p>Nous avons ensuite déterminé deux types d&rsquo;applications pour la suite du traitement :</p><ul><li><p><strong>Transformer</strong> : il s&rsquo;agit d&rsquo;une application qui récupère un message Kafka depuis un topic, qui transforme les données fournies entrée (en les enrichissant éventuellement) et produire un ou plusieurs messages Kafka. Par exemple, à partir d&rsquo;un message de type Video source, nous pouvons produire de petits messages Kafka avec uniquement les données nécessaires à un des écrans (web, mobile ou IPTV).</p></li><li><p><strong>Projector</strong> : il s&rsquo;agit d&rsquo;une application qui récupère un message Kafka depuis un topic et le projette dans une autre base de données comme Redis ou, par exemple, chez un partenaire via des appels APIs.</p></li></ul><p>Étant donné que nous avons détecté ce besoin de <strong>transformation</strong> ou de <strong>projection</strong> assez fréquemment, nous avons également développé une librairie Go open-source permettant de définir une interface simple pour la transformation et la projection des données : <a href=https://github.com/etf1/kafka-transformer target=_blank>https://github.com/etf1/kafka-transformer</a>.</p><p>Dans notre cas, la dernière brique de projection consomme donc les objets de référence construits précédemment par la brique de transformation et construit alors plusieurs nouveaux objets épurés pour <strong>chaque écran</strong> avant de les stocker dans Redis.</p><p>Ainsi, lorsque GraphQL reçoit une requête provenant d&rsquo;une application mobile, par exemple, les données sont <strong>déjà pré-formatées dans Redis</strong> pour leurs besoins spécifiques avec uniquement les données utilisées par les applications mobiles.</p><p>Voici donc un récapitulatif du workflow de données via Kafka :</p><ul><li>Les <strong>oplogs MongoDB</strong> bruts sont envoyés dans un premier topic Kafka configuré avec un <code>cleanup.policy</code> positionné à <code>delete</code> afin de ne pas garder un historique complet.</li><li>Une brique appelée <strong>transformer</strong> vient ensuite lire ce premier topic Kafka, transformer ces oplogs en objets Protobuf de référence et les pousser dans un second topic Kafka, cette fois avec une <code>cleanup.policy</code> de type <code>compact</code> afin que Kafka vienne régulièrement nettoyer le topic et garder uniquement les dernières versions de chaque objet.</li><li>Enfin, une troisième brique appelée <strong>projector</strong> vient lire ce second topic Kafka contenant le message au format Protobuf et projeter, en fonction des cas, la donnée en dehors de Kafka (Redis dans notre cas).</li></ul><p>La dernière étape pour ce lot consistait simplement à brancher certaines requêtes GraphQL directement sur Redis pour, par exemple, récupérer un contenu par son identifiant. Dans les faits nous avons modifié les data-loader existants pour qu&rsquo;ils récupèrent des objets sérialisés en Protobuf dans Redis (très ressemblants à ceux fournis par notre API catalogue GRPC).</p><p>Nous utilisons ainsi Redis comme un cache de données partagé, accessible par toutes nos instances GraphQL et maintenu à jour en temps réel par cette nouvelle architecture reposant sur Kafka.</p><p>Ainsi, si une donnée est modifiée dans notre CMS, dès lors que celle-ci est sauvegardée dans la base MongoDB, les événements sont propulsés au travers de Kafka et des différentes briques applicatives permettant de re-travailler les données dans le but final de venir maintenir à jour Redis.</p><h2 id=lot-2-manipulation-et-stream-de-données>Lot 2 : Manipulation et stream de données</h2><p>L&rsquo;objectif de ce deuxième lot est de confirmer notre capacité à calculer, directement via un stream d&rsquo;un ou plusieurs topics Kafka, des données et les produire dans un topic de sortie qui permet l&rsquo;accès à des informations &ldquo;enrichies&rdquo; (un agrégat par exemple).</p><p>Nous avons donc identifié un cas du côté de l&rsquo;éditorialisation : les <strong>recommandations éditoriales</strong>.</p><p>Sur une page programme, certaines sections présentées sont des recommandations éditoriales, par exemple, une section &ldquo;Interviews&rdquo; sur la page de l&rsquo;émission <a href=https://www.tf1.fr/tmc/quotidien-avec-yann-barthes target=_blank>Quotidien</a> présentant uniquement les dernières vidéos possédant un tag &ldquo;interview&rdquo;.</p><p>Ces sections sont <strong>calculées dynamiquement</strong> en fonction des règles définies dans le CMS et des vidéos attachées au programme et afin de les calculer en se basant sur nos topics Kafka, nous avons besoin de :</p><ul><li><strong>Construire un aggrégat</strong> de toutes les vidéos (pouvant être exploitées) associées au programme,</li><li><strong>Calculer les vidéos</strong> devant faire partie de la sélection éditoriale, en fonction des filtres définis,</li><li><strong>Enrichir l&rsquo;objet</strong> programme avec le calcul effectué et produire l&rsquo;objet enrichi dans un nouveau topic, qui sera à son tour exploité pour la projection dans Redis.</li></ul><p>Pour ce besoin, nous avons commencé par faire des essais avec <a href=https://kafka.apache.org/documentation/streams/ target=_blank>Kafka Streams</a>. Nous avions quelques a priori sur cette technologie du fait qu&rsquo;il s&rsquo;agisse d&rsquo;une librairie Java, que peu de développeurs avaient des connaissances Java et qu&rsquo;il fallait pouvoir gérer ça en production.</p><p>Nous nous sommes tout de même penchés en premier lieu sur cette solution et avons fait des essais concluants ! Nous avons aussi utilisé <a href=https://quarkus.io/guides/kafka-streams target=_blank>Quarkus</a> nous permettant d&rsquo;améliorer les performances de l&rsquo;application et de la déployer dans notre cluster Kubernetes en utilisant simplement le binaire généré en sortie.</p><p>Cependant, ayant besoin de produire des agrégats conséquents pour certains programmes, la consommation en ressources CPU et mémoire était tout de même assez élevée. De plus les temps de traitement initiaux pour construire l&rsquo;ensemble des recommandations éditoriales (et ceux pour la totalité de nos programmes) étaient conséquents (et incompatibles avec nos exigences de performance).</p><p>C&rsquo;est pourquoi nous avons également testé la librairie open-source <a href=https://github.com/lovoo/goka target=_blank>Goka</a>, écrite en Go, permettant de répondre à ce même besoin avec, en plus, le bénéfice de pouvoir écrire les applications en suivant <strong>nos nomenclatures de développement</strong> habituelles. Nous avons aussi observé de meilleures performances en terme d&rsquo;utilisation de ressources mais aussi en temps d&rsquo;exécution.</p><p>La solution Goka est donc maintenant retenue pour ce genre d&rsquo;opérations. Toutefois, Goka ne permet pas de gérer certaines opérations de Kafka Streams comme les <a href=https://kafka.apache.org/11/javadoc/org/apache/kafka/streams/kstream/TimeWindows.html target=_blank>fenêtres de temps</a>, qui nous seront certainement utiles pour de futurs besoins. Nous n&rsquo;excluons donc pas avoir besoin de nous tourner vers Kafka Streams pour certains besoins.</p><p>Ce sujet étant particulièrement intéressant à détailler, nous décrirons prochainement l&rsquo;utilisation de Goka dans un futur article.</p><h2 id=lot-3-données-utilisateur-dans-kafka>Lot 3 : Données utilisateur dans Kafka</h2><p>Lors de l&rsquo;ajout en favoris d&rsquo;un programme ou d&rsquo;une vidéo par un utilisateur ou lorsqu&rsquo;il commence à regarder une vidéo, des données le concernant sont enregistrées afin de pouvoir, par exemple, lui remonter les vidéos qu&rsquo;il a commencées à regarder.</p><p>Ces besoins étaient auparavant gérés par deux micro-services et nous avons décidé de les re-grouper en un seul : user-api</p><p>Concernant les données, elles étaient stockées sur un serveur Elasticsearch (plusieurs centaines de millions d&rsquo;entrées) qui n&rsquo;avait aucune plus value car nous n&rsquo;effectuons pas de recherche full-text et les besoins en terme de requêtage sont très simples : nous récupérons simplement les données pour un utilisateur donné.</p><p>Nous en avons donc également profité pour migrer cette brique qui utilisait autrefois <a href=https://www.rabbitmq.com/ target=_blank>RabbitMQ</a> pour l&rsquo;asynchronisme vers Kafka. Nous avons gagné en stabilité et en persistance de ce côté.</p><p>Côté stockage, nous avons remplacé Elasticsearch par <a href=https://aws.amazon.com/fr/dynamodb/ target=_blank>AWS DynamoDB</a>, base de données NoSQL managée par AWS nous permettant de stocker à moindre coût ce grand nombre d&rsquo;entrées tout en nous permettant de requêter les données qui sont ensuite servies aux fronts.</p><h2 id=conclusion>Conclusion</h2><p>Pour finir, je préciserai que nous utilisons <a href=https://aws.amazon.com/fr/msk/ target=_blank>AWS MSK</a>, version de Kafka managée par AWS qui nous permet de tirer pleinement parti des APIs Kafka.
Nous n&rsquo;avons à ce jour pas identifié de limitation.</p><p>Ces deux premiers lots nous ont permis de nous assurer que Kafka répondait à notre besoin afin de faire évoluer les workflows de données de MYTF1.</p><p>Il reste toutefois de nombreux challenges que nous sommes en train de relever, notamment le fait de pouvoir planifier des événements via Kafka pour, par exemple, faire expirer certaines données ou encore déclencher des événements à des moments précis.</p><p>N&rsquo;hésitez pas à nous contacter si vous souhaitez avoir plus d&rsquo;informations sur cette migration vers Kafka, nous serons ravis d&rsquo;échanger sur ces problématiques.</p><p>Nous reviendrons également plus en détail sur nos briques open-source dans de futurs articles.</p><h2 id=crédits>Crédits</h2><p>L&rsquo;image utilisée pour illustrer cet article est fournie par <a href=https://unsplash.com/photos/3To9V42K0Ag target=_blank>John Baker</a>.</p></article><section id=articleNext class="section nartrow"><h3 class=footer-next-heading>Plus d'articles</h3><div class=footer-spacer></div><div class=next-articles-grid numberofarticles={numberOfArticles}><div class=post-row><a href=/post/2020/team/backend/ class=article-link id=article-link-bigger><div><div class=image-container><img src=/post/2020/team/backend/images/hero.jpg class=article-image></div><div><h2 class=article-title>L'équipe Backend</h2><p class=article-excerpt>Welcome to the other side</p><div class=article-metadata>17 décembre 2020 • 14 minutes</div></div></div></a><a href=/post/2020/architecture/graphql-and-persisted-queries/ class=article-link><div><div class=image-container><img src=/post/2020/architecture/graphql-and-persisted-queries/images/hero-v2.jpg class=article-image></div><div><h2 class=article-title>GraphQL et les persisted queries</h2><p class=article-excerpt>La gestion du cache et l'utilisation des persisted queries avec GraphQL</p><div class=article-metadata>25 novembre 2020 • 5 minutes</div></div></div></a></div></div></section></section><script src=/js/progressBar.js></script><div class=footer-gradient></div><div class="section narrow"><div class=footer-hr></div><div class=footer-container><div class=footer-text><img src=images/logo_tf1lg.png class=footer-logo>TF1 Copyright © 2021 e-TF1</div><div class=social-icon-outer><div class=social-icon-container><a href=https://www.welcometothejungle.com/fr/companies/groupe-tf1><svg class="social-icon-image" viewBox="7 7 26 26" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" alt="Rejoingez nous !" title="Rejoignez nous !"><path d="M31.347322 11.295661C30.1466441 9.31125424 26.6585085 8.44345763 24.1202034 11.0048136 23.9785085 11.1532881 23.7913898 11.6054915 24.3554576 12.2149831c1.181017 1.258305 7.340339 7.7179661 5.0616949 13.36-1.1071186 2.7430508-4.2555932 2.2705084-5.438644-2.2738984C23.4991864 21.4590508 22.7595254 17.6326102 23.468678 13.0393898 23.468678 13.0393898 23.5818983 12.6082034 23.3995254 12.4705763 23.2734237 12.3743051 23.0707119 12.4143051 22.8103729 12.8312542 22.7452881 12.936339 22.7588475 12.9146441 22.6964746 13.0265085L22.6924068 13.0285424c-1.7457627 2.9444068-3.5220339 6.4352542-4.3579661 7.7444068C18.7425763 18.7980339 18.2252881 16.0814237 17.4185085 13.8075254L17.2666441 13.396678C16.5215593 11.4515932 15.6259661 10.1390508 15.3541017 9.84142373 15.3215593 9.80549153 15.2408814 9.73837288 14.9744407 9.73837288H7.42461017c-.528135590000001.0-.4820339.45220342-.31525424.615593219999999 2.00474576 2.3857627 7.84610167 11.3294915 5.13762717 20.1064407C12.0381695 31.1390508 12.3663051 31.4949831 12.9066441 30.7946441c2.3742373-3.0772882 5.32-8.3227119 7.4542373-12.2542373C19.8903729 19.9037966 19.4761356 21.4217627 19.2544407 23.0122712c-.4677966 3.3572881.5247457 5.8827119 1.2508474 7.2081356C20.8768136 30.8990508 21.3696949 31.3139661 22.6456271 31.1241356c6.5071187-.9694915 8.6772882-6.7016949 9.4786441-9.6088136C33.5852881 16.215661 32.5154576 13.2258305 31.347322 11.295661" id="monogram" fill="#000"/></svg></a><span class=hidden>https://www.welcometothejungle.com/fr/companies/groupe-tf1</span>
<a href=https://www.linkedin.com/company/e-tf1/><svg class="social-icon-image" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path fillRule="evenodd" clipRule="evenodd" d="M3.59615 13.125H.871552V4.36523H3.59615V13.125zM2.24847 3.16406c-.42969.0-.80078-.15625-1.11328-.46875-.312498-.3125-.468747-.6836-.468747-1.11328.0-.42969.156249-.800782.468747-1.113281C1.44769.156249 1.81878.0 2.24847.0s.80078.156249 1.11328.468749c.3125.312499.46875.683591.46875 1.113281.0.42968-.15625.80078-.46875 1.11328s-.68359.46875-1.11328.46875zM13.7915 13.125H11.0669V8.84765C11.0669 8.14452 11.0083 7.63671 10.8911 7.32421c-.2148-.52734-.6348-.79101-1.25976-.79101-.625.0-1.06445.23437-1.31836.70312C8.11767 7.58788 8.02001 8.10546 8.02001 8.78905V13.125H5.32471V4.36523H7.93212V5.5664H7.96142C8.15673 5.17578 8.46923 4.85351 8.89892 4.59961c.46875-.3125 1.01562-.46875 1.64058-.46875 1.2696.0 2.1582.40039 2.666 1.20117C13.5962 5.97656 13.7915 6.97265 13.7915 8.3203V13.125z" fill="#73737d"/></svg></a><span class=hidden>https://www.linkedin.com/company/e-tf1/</span>
<a href=https://github.com/etf1><svg class="social-icon-image" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg"><path fillRule="evenodd" clipRule="evenodd" d="M7 0C3.1325.0.0 3.21173.0 7.17706.0 10.3529 2.00375 13.0353 4.78625 13.9863 5.13625 14.0491 5.2675 13.8338 5.2675 13.6454 5.2675 13.4749 5.25875 12.9097 5.25875 12.3087 3.5 12.6406 3.045 11.8691 2.905 11.4653 2.82625 11.259 2.485 10.622 2.1875 10.4516 1.9425 10.317 1.5925 9.98508 2.17875 9.97611 2.73 9.96714 3.12375 10.4964 3.255 10.7118c.63 1.0855 1.63625.7805 2.03875.5921C5.355 10.8374 5.53875 10.5234 5.74 10.3439 4.1825 10.1645 2.555 9.54549 2.555 6.80026c0-.7805.27125-1.42644.7175-1.92883C3.2025 4.692 2.9575 3.95635 3.3425 2.96951c0 0 .58625-.1884 1.925.73565.56-.16149 1.155-.24223 1.75-.24223s1.19.08074 1.75.24223c1.3388-.93302 1.925-.73565 1.925-.73565C11.0775 3.95635 10.8325 4.692 10.7625 4.87143 11.2087 5.37382 11.48 6.01079 11.48 6.80026c0 2.7542-1.63625 3.36424-3.19375 3.54364C8.54 10.5682 8.75875 10.9988 8.75875 11.6717 8.75875 12.6316 8.75 13.4032 8.75 13.6454 8.75 13.8338 8.88125 14.0581 9.23125 13.9863 11.9963 13.0353 14 10.3439 14 7.17706 14 3.21173 10.8675.0 7 0z" fill="#73737d"/></svg></a><span class=hidden>https://github.com/etf1</span>
<a href=https://tech.tf1.fr/index.xml><svg class="social-icon-image" style="width:1em;height:1em;vertical-align:middle;fill:currentColor;overflow:hidden" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg"><path d="M329.142857 768q0 45.714286-32 77.714286t-77.714286 32-77.714285-32-32-77.714286 32-77.714286 77.714285-32 77.714286 32 32 77.714286zm292.571429 70.285714q1.142857 16-9.714286 27.428572-10.285714 12-26.857143 12H508q-14.285714.0-24.571429-9.428572T472 844.571429q-12.571429-130.857143-105.428571-223.714286T142.857143 515.428571Q128.571429 514.285714 119.142857 504T109.714286 479.428571V402.285714q0-16.571429 12-26.857143 9.714286-9.714286 24.571428-9.714285h2.857143q91.428571 7.428571 174.857143 46T472 515.428571q65.142857 64.571429 103.714286 148t46 174.857143zm292.571428 1.142857Q915.428571 854.857142 904 866.285714q-10.285714 11.428571-26.285714 11.428572H796q-14.857143.0-25.428571-10t-11.142858-24.285715Q752.571428 720.571428 701.714286 610T569.428571 418t-192-132.285714T144 227.428571q-14.285714-.571429-24.285714-11.142857t-10-24.857143V109.714286q0-16 11.428571-26.285715 10.285714-10.285714 25.142857-10.285714H148q149.714286 7.428571 286.571429 68.571429t243.142857 168q106.857143 106.285714 168 243.142857t68.571428 286.571428z"/></svg></a><span class=hidden>https://tech.tf1.fr/index.xml</span></div></div></div></div></div><script src=/js/prism.js></script></body></html>